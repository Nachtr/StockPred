{
  "cells": [
    {
      "metadata": {
        "id": "8a77807f92f26ee"
      },
      "cell_type": "markdown",
      "source": [
        "Stock Prediction Project"
      ],
      "id": "8a77807f92f26ee"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ta streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Bz23RvWc66T",
        "outputId": "947aa673-7dd4-432b-98ab-79589282d63d"
      },
      "id": "9Bz23RvWc66T",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ta in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.52.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from ta) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from ta) (2.2.2)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.12.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->ta) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.11.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.29.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->ta) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-04T11:53:17.298660Z",
          "start_time": "2025-12-04T11:53:17.294910Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbc121e30a2defb3",
        "outputId": "c7de14a5-b261-4059-a2a3-4525882e86bf"
      },
      "cell_type": "code",
      "source": [
        "#Import libs\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pkg_resources import non_empty_lines\n",
        "import joblib\n",
        "import io\n",
        "\n",
        "\n",
        "#indicators\n",
        "from ta.momentum import RSIIndicator, StochasticOscillator, ROCIndicator, WilliamsRIndicator\n",
        "from ta.trend import MACD, ADXIndicator, EMAIndicator, CCIIndicator, AroonIndicator\n",
        "from ta.volatility import BollingerBands, AverageTrueRange\n",
        "from ta.volume import OnBalanceVolumeIndicator, MFIIndicator\n",
        "\n",
        "#ML stuffs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Visual + Warn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "# Finance data\n",
        "import yfinance as yf\n",
        "from datetime import datetime\n",
        "\n",
        "# Streamlit\n",
        "import streamlit as st\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ],
      "id": "fbc121e30a2defb3",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "execution_count": 57
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-04T11:53:18.073522Z",
          "start_time": "2025-12-04T11:53:18.070696Z"
        },
        "id": "260f66356d827ef0"
      },
      "cell_type": "code",
      "source": [
        "# Configs from necessary research\n",
        "\n",
        "TICKERS = [ # I decided to remove tesla since its movement is usually based on the CEO and news...\n",
        "    'SPY', # INDEX\n",
        "    'QQQ',\n",
        "    'AAPL', # Heavy hitter for S&P\n",
        "    'NVDA',\n",
        "    'MSFT', # Heavy hitter for S&P\n",
        "    'JPM',\n",
        "    'AMZN', # Heavy hitter for S&P\n",
        "    'XOM',\n",
        "    'BAC',\n",
        "    'JNJ'\n",
        "]\n",
        "\n",
        "# For our start dates, we are going to go back 10 years.\n",
        "\n",
        "START_DATE = '2013-01-01'\n",
        "END_DATE = '2025-05-05'\n",
        "\n",
        "# 5 day forward predictions\n",
        "PREDICTION_HORIZON = 5\n",
        "\n",
        "# Cross-Val\n",
        "N_SPLITS = 5\n",
        "\n",
        "# Gap\n",
        "GAP = 5\n",
        "\n",
        "\n",
        "# Configs for graphs\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)"
      ],
      "id": "260f66356d827ef0",
      "outputs": [],
      "execution_count": 58
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-04T11:56:58.592765Z",
          "start_time": "2025-12-04T11:56:58.578473Z"
        },
        "id": "7f9bd395935dade"
      },
      "cell_type": "code",
      "source": [
        "# Helper functions\n",
        "\n",
        "def download_stock_data(ticker, start, end):\n",
        "    try:\n",
        "        print(f\"Downloading {ticker} data from {start} to {end}...\")\n",
        "        # DEBUG: Added multi_level_index=False to prevent 2D shape errors\n",
        "        df = yf.download(ticker, start=start, end=end, auto_adjust=True, progress=False, multi_level_index=False)\n",
        "\n",
        "        if df.empty:\n",
        "            print(f\"No data available for {ticker}...\")\n",
        "            return None\n",
        "\n",
        "        # Flatten columns if yfinance returns a MultiIndex\n",
        "        if isinstance(df.columns, pd.MultiIndex):\n",
        "            df.columns = df.columns.get_level_values(0)\n",
        "\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {ticker}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def add_tier1_indicators(df): # these will provide 80% of our predictive power\n",
        "    # RSI is what 90% of traders use. This stands for relative strength index\n",
        "    df['rsi'] = RSIIndicator(close = df['Close'], window = 14).rsi()\n",
        "\n",
        "    # MACD is the next ind we plan to implement.\n",
        "    macd = MACD(close=df['Close'], window_slow=26, window_fast=12, window_sign=9)\n",
        "    df['macd_line'] = macd.macd()\n",
        "    df['macd_signal'] = macd.macd_signal()\n",
        "    df['macd_hist'] = macd.macd_diff()\n",
        "\n",
        "    # Bollinger Bands\n",
        "    bb = BollingerBands(close=df['Close'], window=20, window_dev = 2)\n",
        "    df['bb_width'] = bb.bollinger_wband()\n",
        "    df['bb_percent'] = bb.bollinger_pband()\n",
        "\n",
        "    # ADX (Average Directional Index)\n",
        "    adx = ADXIndicator(high=df['High'], low=df['Low'], close=df['Close'], window=14)\n",
        "    df['adx'] = adx.adx()\n",
        "    df['adx_pos'] = adx.adx_pos()\n",
        "    df['adx_neg'] = adx.adx_neg()\n",
        "    df['adx_diff'] = df['adx_pos'] - df['adx_neg'] # this is our directional component for better analysis\n",
        "\n",
        "    # ATR (Average True Range)\n",
        "    atr = AverageTrueRange(high=df['High'], low=df['Low'], close=df['Close'], window=14)\n",
        "    df['atr_norm'] = atr.average_true_range() / df['Close']\n",
        "\n",
        "    # This one must be normalized ^^^\n",
        "    # These should be about 10 features\n",
        "\n",
        "\n",
        "    return df\n",
        "\n",
        "def add_tier2_indicators(df):\n",
        "    # These are recommended indicators that will increase accuracy and add diversity to the set\n",
        "    # This should add around 8 features\n",
        "\n",
        "    # Stoch Osci\n",
        "    stoch = StochasticOscillator(high=df['High'], low=df['Low'], close=df['Close'], window=14, smooth_window=3)\n",
        "    df['stoch_k'] = stoch.stoch()\n",
        "    df['stoch_d'] = stoch.stoch_signal()\n",
        "\n",
        "\n",
        "    # OBV (on balance volume) - This is should use rate of change and not absolute\n",
        "    obv = OnBalanceVolumeIndicator(close = df['Close'], volume = df['Volume'])\n",
        "    df['obv_roc'] = obv.on_balance_volume().pct_change(5)\n",
        "\n",
        "    # Williams %R\n",
        "    df['williams_r'] = WilliamsRIndicator(high=df['High'], low=df['Low'], close = df['Close'], lbp = 14).williams_r()\n",
        "\n",
        "    # EMA\n",
        "    ema_12 = EMAIndicator(close = df['Close'], window=12).ema_indicator()\n",
        "    ema_26 = EMAIndicator(close = df['Close'], window=26).ema_indicator()\n",
        "    df['ema_ratio'] = ema_12 / ema_26\n",
        "\n",
        "    # ROC (Rate of Change)\n",
        "    df['roc_5'] = ROCIndicator(close = df['Close'], window = 5).roc()\n",
        "    df['roc_10'] = ROCIndicator(close = df['Close'], window = 10).roc()\n",
        "    return df\n",
        "\n",
        "def add_tier3_indicators(df):\n",
        "    # these are optional extra that will add 6 additional features to help improve acc\n",
        "\n",
        "    # CCI Comod chan index\n",
        "    df['cci'] = CCIIndicator(high = df['High'], low = df['Low'], close = df['Close'], window = 20).cci()\n",
        "\n",
        "    # MFI money flow index\n",
        "    df['mfi'] = MFIIndicator(high = df['High'], low = df['Low'], close = df['Close'], volume = df['Volume'], window = 14).money_flow_index()\n",
        "\n",
        "    # Aroon Indic\n",
        "    aroon = AroonIndicator(high=df['High'], low=df['Low'], window=25)\n",
        "    df['aroon_indicator'] = aroon.aroon_indicator()\n",
        "\n",
        "    return df\n",
        "\n",
        "# I needed to add this here so that we can compare what happened 5 days ago vs this frame...\n",
        "def add_lagged_features(df):\n",
        "  target_features = ['rsi', 'macd_hist', 'adx_diff', 'obv_roc']\n",
        "\n",
        "  for col in target_features:\n",
        "    if col in df.columns:\n",
        "      df[f'{col}_lag_5'] = df[col].shift(5)\n",
        "      df[f'{col}_change_5'] = df[col] - df[f'{col}_lag_5']\n",
        "\n",
        "  # Price momentum (returns)\n",
        "  df['pct_change_3'] = df['Close'].pct_change(3)\n",
        "  df['pct_change_5'] = df['Close'].pct_change(5)\n",
        "\n",
        "  # Volatility context\n",
        "  df['volatility_5'] = df['Close'].pct_change().rolling(5).std()\n",
        "  return df\n",
        "\n",
        "def add_all_indicators(df, tier = 'tier2'):\n",
        "    # With this function we should make sure to add technical indicators based on the tier that is selected\n",
        "\n",
        "    '''\n",
        "    :param tier: tier1 (10 feat)\n",
        "    :param tier: tier2 (18 feat)\n",
        "    :param tier: tier3 (24 feat)\n",
        "    '''\n",
        "\n",
        "    df = add_tier1_indicators(df)\n",
        "\n",
        "    if tier in ['tier2', 'tier3']:\n",
        "        df = add_tier2_indicators(df)\n",
        "    if tier == 'tier3':\n",
        "        df = add_tier3_indicators(df)\n",
        "\n",
        "    df = add_lagged_features(df)\n",
        "\n",
        "    return df\n",
        "\n",
        "def create_target_var(df, horizon=5):\n",
        "    # We need to have a function that creates a binary target. If price goes up compared to the horizon days, return 1.\n",
        "\n",
        "    df['target'] = (df['Close'].shift(-horizon) > df['Close']).astype(int)\n",
        "    return df\n",
        "\n",
        "def get_feature_columns(tier = 'tier2'):\n",
        "    # Return feature column names based on the selected tier\n",
        "\n",
        "    base_features = [\n",
        "      'rsi',\n",
        "      'macd_hist',\n",
        "      'bb_width',\n",
        "      'bb_percent',\n",
        "      'adx_diff',\n",
        "      'atr_norm',\n",
        "      'rsi_change_5',\n",
        "      'volatility_5',\n",
        "      'pct_change_5'\n",
        "    ]\n",
        "\n",
        "    tier2_features = [\n",
        "      'stoch_k',\n",
        "      'obv_roc',\n",
        "      'ema_ratio',\n",
        "      'roc_10'\n",
        "    ]\n",
        "\n",
        "    tier3_features = [\n",
        "      'cci',\n",
        "      'mfi',\n",
        "      'aroon_indicator'\n",
        "    ]\n",
        "\n",
        "    if tier == 'tier1':\n",
        "        return base_features\n",
        "    elif tier == 'tier2':\n",
        "        return base_features + tier2_features\n",
        "    else:\n",
        "        return base_features + tier2_features + tier3_features\n",
        "\n",
        "def check_feature_correlation(df, feature_cols, threshold=0.9):\n",
        "    \"\"\"\n",
        "    With this function we need to check for highly correlated features and recommend removal. From my research document, I have found that correlation >9.9 should be avoided and removed.\n",
        "    \"\"\"\n",
        "\n",
        "    corr_matrix = df[feature_cols].corr().abs()\n",
        "\n",
        "    high_corr_pairs = []\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i+1, len(corr_matrix.columns)):\n",
        "            if corr_matrix.iloc[i, j] > threshold:\n",
        "                high_corr_pairs.append({\n",
        "                    'feature1':corr_matrix.columns[i],\n",
        "                    'feature2':corr_matrix.columns[j],\n",
        "                    'correlation':corr_matrix.iloc[i, j]\n",
        "                })\n",
        "\n",
        "    if high_corr_pairs:\n",
        "        print(f\"\\n High correlation pairs found (>{threshold})\")\n",
        "        for pair in high_corr_pairs:\n",
        "            print(f\" - {pair['feature1']} <-> {pair['feature2']}: {pair['correlation']:.3f}\")\n",
        "    else:\n",
        "        print(f\"\\n No highly correlated features (>{threshold})\")\n",
        "    return high_corr_pairs\n",
        "\n",
        "def train_xgboost_model(X_train, X_test, y_train, y_test, verbose=False):\n",
        "    \"\"\"\n",
        "    Train with XGBoost with reg params\n",
        "    \"\"\"\n",
        "\n",
        "    model = xgb.XGBClassifier(\n",
        "        max_depth=6,\n",
        "        min_child_weight=3,\n",
        "        subsample=0.7,\n",
        "        colsample_bytree=0.7,\n",
        "        learning_rate=0.05,\n",
        "        n_estimators=100,\n",
        "        reg_lambda=1.0,\n",
        "        random_state=42,\n",
        "        eval_metric='logloss',\n",
        "        use_label_encoder=False,\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train, verbose=verbose)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    return model, y_pred, accuracy\n",
        "\n",
        "def train_random_forest_model(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Train with random forest with reg params\n",
        "    :param X_train:\n",
        "    :param X_test:\n",
        "    :param y_train:\n",
        "    :param y_test:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=10,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        max_features='sqrt',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    return model, y_pred, accuracy\n",
        "\n",
        "def train_and_evaluate(df, ticker, tier='tier2', model_type='xgboost'):\n",
        "    # Train model with time-series cross val and comp eval\n",
        "    feature_cols = get_feature_columns(tier)\n",
        "    df_clean = df.dropna()\n",
        "\n",
        "    print(f\"\\nTraining ENSEMBLE (XGB + RF) for {ticker}...\")\n",
        "\n",
        "    tscv = TimeSeriesSplit(n_splits = N_SPLITS, gap = GAP)\n",
        "    fold_accuracies = []\n",
        "    feature_importances = []\n",
        "\n",
        "\n",
        "    # Define the two models\n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "        max_depth=6,\n",
        "        learning_rate = 0.05,\n",
        "        n_estimators = 100,\n",
        "        eval_metric = 'logloss',\n",
        "        use_label_encoder = False,\n",
        "        random_state = 42\n",
        "    )\n",
        "\n",
        "    rf_model = RandomForestClassifier(\n",
        "        n_estimators = 100,\n",
        "        max_depth = 8,\n",
        "        min_samples_leaf=4,\n",
        "        random_state = 42,\n",
        "        n_jobs = -1\n",
        "    )\n",
        "\n",
        "    # Voting Ensemble\n",
        "    ensemble = VotingClassifier(\n",
        "        estimators = [('xgb', xgb_model), ('rf', rf_model)],\n",
        "        voting = 'soft'\n",
        "    )\n",
        "\n",
        "    for train_idx, test_idx in tscv.split(df_clean):\n",
        "      X_train = df_clean.iloc[train_idx][feature_cols]\n",
        "      y_train = df_clean.iloc[train_idx]['target']\n",
        "      X_test = df_clean.iloc[test_idx][feature_cols]\n",
        "      y_test = df_clean.iloc[test_idx]['target']\n",
        "\n",
        "      # Scale\n",
        "      scaler = StandardScaler()\n",
        "      X_train_scaled = scaler.fit_transform(X_train)\n",
        "      X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "      # Train Ensemble\n",
        "      ensemble.fit(X_train_scaled, y_train)\n",
        "\n",
        "      # Prediction\n",
        "      y_pred = ensemble.predict(X_test_scaled)\n",
        "      acc = accuracy_score(y_test, y_pred)\n",
        "      fold_accuracies.append(acc)\n",
        "\n",
        "      # Extract Feature Importance\n",
        "      feature_importances.append(ensemble.named_estimators_['rf'].feature_importances_)\n",
        "\n",
        "    avg_acc = np.mean(fold_accuracies)\n",
        "    print(f\"Average Accuracy: {avg_acc:.4f}\")\n",
        "\n",
        "    # Final Training\n",
        "    scaler_final = StandardScaler()\n",
        "    X_all_scaled = scaler_final.fit_transform(df_clean[feature_cols])\n",
        "    ensemble.fit(X_all_scaled, df_clean['target'])\n",
        "\n",
        "    return {\n",
        "        'ticker': ticker,\n",
        "        'model_type': model_type,\n",
        "        'tier': tier,\n",
        "        'model': ensemble,\n",
        "        'avg_accuracy': avg_acc,\n",
        "        'fold_accuracies': fold_accuracies,\n",
        "        'feature_importance': dict(zip(feature_cols, np.mean(feature_importances, axis = 0))),\n",
        "        'min_accuracy': min(fold_accuracies),\n",
        "        'max_accuracy': max(fold_accuracies),\n",
        "        'std_accuracy': np.std(fold_accuracies)\n",
        "    }\n",
        "\n",
        ""
      ],
      "id": "7f9bd395935dade",
      "outputs": [],
      "execution_count": 59
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-04T11:56:59.875429Z",
          "start_time": "2025-12-04T11:56:59.865695Z"
        },
        "id": "711e8cf8692650bd"
      },
      "cell_type": "code",
      "source": [
        "def plot_feature_importance(results, save_dir='./plots'):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    importance_df = pd.DataFrame([\n",
        "        {'feature' : k, 'importance' : v}\n",
        "        for k, v in results['feature_importance'].items()\n",
        "    ]).sort_values('importance', ascending=False)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    colors = sns.color_palette(\"viridis\", len(importance_df))\n",
        "    sns.barplot(data=importance_df, y='feature', x='importance', palette=colors)\n",
        "    plt.title(f\"Feature Importance - {results['ticker']} ({results['model_type'].upper()})\\n\"\n",
        "              f\"Accuracy: {results['avg_accuracy']*100:.2f}%\", fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Importance Score', fontsize=12)\n",
        "    plt.ylabel('Feature', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    filename = f\"{save_dir}/feature_importance_{results['ticker']}_{results['model_type']}.png\"\n",
        "    plt.savefig(filename, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return filename\n",
        "\n",
        "def plot_accuracy_comparison(all_results, save_dir='./plots'):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    comparison_data = []\n",
        "    for result in all_results:\n",
        "        comparison_data.append({\n",
        "            'Ticker': result['ticker'],\n",
        "            'Accuracy': result['avg_accuracy'] * 100,\n",
        "            'Model': result['model_type'].upper()\n",
        "        })\n",
        "\n",
        "    df_comp = pd.DataFrame(comparison_data)\n",
        "\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    ax = sns.barplot(data=df_comp, x='Ticker', y='Accuracy', hue='Model', palette='Set2')\n",
        "\n",
        "    plt.axhline(y=60, color='red', linestyle='--', linewidth=2, label='60% Target')\n",
        "\n",
        "    for container in ax.containers:\n",
        "        ax.bar_label(container, fmt='%.1f%%', padding=3)\n",
        "\n",
        "    plt.title('Model Accuracy Comparison Across Stocks\\n5-Day Price Direction Prediction',\n",
        "              fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Stock Ticker', fontsize=12)\n",
        "    plt.ylabel('Accuracy (%)', fontsize=12)\n",
        "    plt.legend(title='Model Type', fontsize=10)\n",
        "    plt.ylim(40, max(df_comp['Accuracy'].max() + 5, 70))\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    filename = f\"{save_dir}/accuracy_comparison.png\"\n",
        "    plt.savefig(filename, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return filename\n",
        "\n",
        "def save_detailed_results(all_results, filename = 'detailed_results.csv'):\n",
        "    detailed_data = []\n",
        "\n",
        "    for result in all_results:\n",
        "        for fold_num, fold_acc in enumerate(result['fold_accuracies'], 1):\n",
        "            detailed_data.append({\n",
        "                'Ticker': result['ticker'],\n",
        "                'Model': result['model_type'],\n",
        "                'Tier': result['tier'],\n",
        "                'Fold': fold_num,\n",
        "                'Accuracy': fold_acc * 100,\n",
        "                'Average_Accuracy': result['avg_accuracy'] * 100,\n",
        "                'Std_Dev': result['std_accuracy'] * 100,\n",
        "                'Min_Accuracy': result['min_accuracy'] * 100,\n",
        "                'Max_Accuracy': result['max_accuracy'] * 100\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(detailed_data)\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"\\nDetailed results saved to: {filename}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def print_final_summary(all_results):\n",
        "    \"\"\"Print comprehensive final summary.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"FINAL SUMMARY - ALL STOCKS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    summary_data = []\n",
        "    for r in all_results:\n",
        "        summary_data.append({\n",
        "            'Ticker': r['ticker'],\n",
        "            'Model': r['model_type'].upper(),\n",
        "            'Tier': r['tier'].upper(),\n",
        "            'Avg Acc': f\"{r['avg_accuracy']*100:.2f}%\",\n",
        "            'Std Dev': f\"{r['std_accuracy']*100:.2f}%\",\n",
        "            'Min': f\"{r['min_accuracy']*100:.2f}%\",\n",
        "            'Max': f\"{r['max_accuracy']*100:.2f}%\",\n",
        "            'Target Met': '✓' if r['avg_accuracy'] >= 0.60 else '✗'\n",
        "        })\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    print(summary_df.to_string(index=False))\n",
        "\n",
        "    overall_avg = np.mean([r['avg_accuracy'] for r in all_results])\n",
        "    overall_std = np.std([r['avg_accuracy'] for r in all_results])\n",
        "    stocks_above_60 = sum(1 for r in all_results if r['avg_accuracy'] >= 0.60)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Overall Statistics\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Average Accuracy Across All Stocks: {overall_avg*100:.2f}% (±{overall_std*100:.2f}%)\")\n",
        "    print(f\"Stocks Meeting 60% Target: {stocks_above_60}/{len(all_results)}\")\n",
        "    print(f\"Best Performing Stock: {max(all_results, key=lambda x: x['avg_accuracy'])['ticker']} \"\n",
        "          f\"({max(r['avg_accuracy'] for r in all_results)*100:.2f}%)\")\n",
        "    print(f\"Worst Performing Stock: {min(all_results, key=lambda x: x['avg_accuracy'])['ticker']} \"\n",
        "          f\"({min(r['avg_accuracy'] for r in all_results)*100:.2f}%)\")\n",
        "\n",
        "    if overall_avg >= 0.60:\n",
        "        print(f\"\\nSuccess: Overall average EXCEEDS 60% target!\")\n",
        "    else:\n",
        "        gap = (0.60 - overall_avg) * 100\n",
        "        print(f\"\\nOverall average is {gap:.2f}% below 60% target\")\n",
        "        print(f\"Recommendations:\")\n",
        "        print(f\"   - Try tier3 indicators for more features\")\n",
        "        print(f\"   - Test Random Forest model as alternative\")\n",
        "        print(f\"   - Consider ensemble methods (combine XGBoost + RF)\")\n",
        "\n",
        "    return summary_df\n"
      ],
      "id": "711e8cf8692650bd",
      "outputs": [],
      "execution_count": 60
    },
    {
      "cell_type": "markdown",
      "source": [
        "I want to use StreamLit to create an interactive dashboard like page for the charts"
      ],
      "metadata": {
        "id": "2KJPJFWuRIV4"
      },
      "id": "2KJPJFWuRIV4"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "\"\"\"\n",
        "I am restating and adding these imports so that it copies to the app.py file for streamlit\n",
        "\"\"\"\n",
        "\n",
        "# General\n",
        "import streamlit as st\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Technical Indicators\n",
        "from ta.momentum import RSIIndicator\n",
        "from ta.volatility import BollingerBands\n",
        "from ta.trend import MACD\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "import xgboost as xgb\n",
        "\n",
        "@st.cache_data\n",
        "def download_data(ticker, start, end):\n",
        "  \"\"\"\n",
        "  With this, we will cache to prevent re-fetching\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    df = yf.download(ticker, start = start, end = end, auto_adjust = True, progress = False, multi_level_index = False)\n",
        "    return df\n",
        "  except Exception as e:\n",
        "    st.error(f\"Error downloading {ticker}: {e}\")\n",
        "    return pd.DataFrame()\n",
        "\n",
        "def add_features(df, spy_data = None):\n",
        "  \"\"\"\n",
        "  Recreate the exact features used in the notebook below and above\n",
        "  \"\"\"\n",
        "\n",
        "  df = df.copy()\n",
        "\n",
        "  # trend ind\n",
        "  df['RSI'] = RSIIndicator(close = df['Close'], window = 14).rsi()\n",
        "  bb = BollingerBands(close = df['Close'], window = 20, window_dev = 2)\n",
        "  df['BB_Width'] = bb.bollinger_wband()\n",
        "  df['BB_Pct'] = bb.bollinger_pband()\n",
        "\n",
        "  macd = MACD(close = df['Close'])\n",
        "  df['MACD_Diff'] = macd.macd_diff()\n",
        "\n",
        "  # Market context for spy like we did\n",
        "  if spy_data is not None:\n",
        "    # Merge on index (date)\n",
        "    df = df.merge(spy_data, left_index = True, right_index = True, how = 'left')\n",
        "\n",
        "    # RS (alpha)\n",
        "    # We need to ask if the stock is beating the market recently\n",
        "    df['Rel_Strength'] = df['Close'].pct_change(20) - df['SPY_Close'].pct_change(20)\n",
        "\n",
        "  # Volat and Trend\n",
        "  sma_20 = df['Close'].rolling(window = 20).mean()\n",
        "  df['Trend_Slope'] = (sma_20 - sma_20.shift(5)) / sma_20.shift(5)\n",
        "\n",
        "  return df.fillna(method = 'ffill').fillna(0)\n",
        "\n",
        "  # Streamlit layout & execution\n",
        "st.set_page_config(page_title = \"Stock Dashboard + Sniper Predictions\", layout = \"wide\")\n",
        "\n",
        "# Sidebar\n",
        "st.sidebar.title(\"Sniper Config\")\n",
        "\n",
        "TICKER = st.sidebar.text_input(\"Stock Ticker\", value = \"SPY\").upper()\n",
        "START_DATE = st.sidebar.date_input(\"Start Date\", value = pd.to_datetime(\"2015-01-01\"))\n",
        "END_DATE = st.sidebar.date_input(\"End Date\", value = pd.to_datetime(\"today\"))\n",
        "HORIZON = st.sidebar.selectbox(\"Prediction Horizon (DAYS)\", [5, 1, 3, 10], index = 0)\n",
        "CONFIDENCE = st.sidebar.slider(\"Sniper Threshold (Confidence in trade)\", 0.55, 0.90, 0.70, 0.60)\n",
        "\n",
        "st.sidebar.markdown(\"---\")\n",
        "st.sidebar.info(\n",
        "    \"**Requirements for Assignment:**\\n\"\n",
        "    \"- Data Exploration\\n\"\n",
        "    \"- Scikit-Learn Pipeline\\n\"\n",
        "    \"- Model Evaluation\\n\"\n",
        "    \"- Serialization (.pkl files)\"\n",
        ")\n",
        "\n",
        "# Main Page\n",
        "st.title(f\"{TICKER} Prediction Dashboard\")\n",
        "st.markdown(f\"\"\"\n",
        "The goal of this dashboard is to show the **Ensemble Model (using XHBoost and Random Forest)** to predict if {TICKER} will close **HIGHER** or **LOWER**\n",
        "{HORIZON} days from now. It applies a **confidence threshold** of {CONFIDENCE*100}% to filter out low-quality trade setups, and predictions.\n",
        "\"\"\")\n",
        "\n",
        "# Logic for the buttons and everything else to work\n",
        "if st.button(\"Run\"):\n",
        "  with st.spinner(f\"Training Model for {TICKER}...\"):\n",
        "    spy_raw = download_data('SPY', START_DATE, END_DATE)\n",
        "    stock_raw = download_data(TICKER, START_DATE, END_DATE)\n",
        "\n",
        "    if len(stock_raw) > 200:\n",
        "      # Processing data\n",
        "      spy_feats = pd.DataFrame(index = spy_raw.index)\n",
        "      spy_feats['SPY_Close'] = spy_raw['Close']\n",
        "      spy_feats['SPY_RSI'] = RSIIndicator(close = spy_raw['Close'], window = 14).rsi()\n",
        "      spy_feats['SPY_Trend'] = (spy_raw['Close'] > spy_raw['Close'].rolling(200).mean()).astype(int)\n",
        "\n",
        "      df_full = add_features(stock_raw, spy_feats)\n",
        "      df_full['Target'] = (df_full['Close'].shift(-HORIZON) > df_full['Close']).astype(int)\n",
        "\n",
        "      df_predict = df_full.iloc[-HORIZON:].copy()\n",
        "      df_train = df_full.dropna().copy()\n",
        "\n",
        "      # Define pipeline\n",
        "      feature_cols = ['RSI', 'BB_Width', 'BB_Pct', 'MACD_Diff', 'Trend_Slope', 'Rel_Strength', 'SPY_RSI', 'SPY_Trend']\n",
        "      X = df_train[feature_cols]\n",
        "      y = df_train['Target']\n",
        "\n",
        "      ensemble = VotingClassifier(\n",
        "          estimators=[\n",
        "            ('xgb', xgb.XGBClassifier(max_depth=4, learning_rate=0.03, n_estimators=150, eval_metric='logloss', random_state=42)),\n",
        "            ('rf', RandomForestClassifier(n_estimators=150, max_depth=6, min_samples_leaf=5, random_state=42, n_jobs=-1))\n",
        "          ],\n",
        "          voting='soft'\n",
        "      )\n",
        "\n",
        "      pipeline = Pipeline([('scaler', StandardScaler()), ('ensemble', ensemble)])\n",
        "\n",
        "      # Training\n",
        "      tscv = TimeSeriesSplit(n_splits=5, gap=HORIZON)\n",
        "      pipeline.fit(X, y)\n",
        "\n",
        "      # Predict\n",
        "      last_row = df_predict.iloc[[-1]][feature_cols]\n",
        "      next_prob = pipeline.predict_proba(last_row)[0, 1]\n",
        "      last_price = df_predict.iloc[-1]['Close']\n",
        "\n",
        "\n",
        "      # Display results\n",
        "      col1, col2 = st.columns(2)\n",
        "      col1.metric(\"Current Price\", f\"${last_price:.2f}\")\n",
        "      col2.metric(\"Prediction Probability (^ Up ^)\", f\"{next_prob*100:.2f}%\")\n",
        "\n",
        "      if next_prob > CONFIDENCE:\n",
        "        st.success(\"Sniper Signal : **BUY**\")\n",
        "      elif next_prob < (1 - CONFIDENCE):\n",
        "        st.error(\"Sniper Signal : **SELL**\")\n",
        "      else:\n",
        "        st.info(\"**NO SIGNAL (HOLD)**\")\n",
        "\n",
        "      st.subheader(\"Feature Importance\")\n",
        "      rf_model = pipeline.named_steps['ensemble'].named_estimators_['rf']\n",
        "      importances = pd.DataFrame({\n",
        "          'Feature' : feature_cols,\n",
        "          'Importance' : rf_model.feature_importances_\n",
        "      }).sort_values(\"Importance\", ascending = False)\n",
        "\n",
        "      fig, ax = plt.subplots(figsize = (10,4))\n",
        "      sns.barplot(data = importances, x = 'Importance', y = 'Feature', palette = 'viridis', ax = ax)\n",
        "      st.pyplot(fig)\n",
        "\n",
        "      # Download button for results\n",
        "      st.markdown(\"---\")\n",
        "      buffer = io.BytesIO()\n",
        "      joblib.dump(pipeline, buffer)\n",
        "      buffer.seek(0)\n",
        "      st.download_button(\"Download Trained Pipeline (.pkl)\", buffer, f\"{TICKER}_model.pkl\")\n",
        "    else:\n",
        "      st.error(\"Not enough data to train...\")\n",
        "else:\n",
        "  st.info(\"Set your parameters and click 'Run' to start.\")"
      ],
      "metadata": {
        "id": "tKLG4sdxRH8J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e48b2415-c4ae-4021-aaef-12058f24af9d"
      },
      "id": "tKLG4sdxRH8J",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Cloudflare Tunnel (More reliable than localtunnel) and will allow us to launch the board\n",
        "!wget -q -O cloudflared-linux-amd64 https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared-linux-amd64\n",
        "\n",
        "# Run Streamlit in the background\n",
        "print(\"Starting Streamlit...\")\n",
        "!streamlit run app.py &>/content/streamlit.log &\n",
        "\n",
        "# Run Cloudflare Tunnel in the background\n",
        "print(\"Starting Cloudflare Tunnel...\")\n",
        "!./cloudflared-linux-amd64 tunnel --url http://localhost:8501 &>/content/tunnel.log &\n",
        "\n",
        "import time\n",
        "import re\n",
        "\n",
        "print(\"Waiting for tunnel to initialize...\")\n",
        "time.sleep(5) # Give it time\n",
        "\n",
        "# Extract the URL\n",
        "found_url = False\n",
        "try:\n",
        "    with open('/content/tunnel.log', 'r') as f:\n",
        "        log_content = f.read()\n",
        "        # Find the trycloudflare.com URL\n",
        "        url_match = re.search(r'https://[a-zA-Z0-9-]+\\.trycloudflare\\.com', log_content)\n",
        "        if url_match:\n",
        "            public_url = url_match.group(0)\n",
        "            print(f\"\\nDASHBOARD URL: {public_url}\")\n",
        "            found_url = True\n",
        "        else:\n",
        "            print(\"\\nCould not find URL yet. Printing last 10 lines of log:\")\n",
        "            print(log_content[-500:])\n",
        "except Exception as e:\n",
        "    print(f\"Error reading logs: {e}\")\n",
        "\n",
        "if not found_url:\n",
        "    print(\"\\nIf no URL appeared, wait 10 seconds and run this cell again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCIF-3fgOY1P",
        "outputId": "384c1ccd-2487-4e49-ba82-894ce825c15a"
      },
      "id": "lCIF-3fgOY1P",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cloudflared-linux-amd64: Text file busy\n",
            "Starting Streamlit...\n",
            "Starting Cloudflare Tunnel...\n",
            "Waiting for tunnel to initialize...\n",
            "\n",
            "DASHBOARD URL: https://served-molecular-raymond-healthcare.trycloudflare.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional helper functions\n",
        "\n",
        "def add_trend_features(df, spy_feats):\n",
        "  df = df.merge(spy_feats, left_index = True, right_index = True, how = 'left')\n",
        "  sma_20 = df['Close'].rolling(window = 20).mean()\n",
        "  df['Trend_Slope'] = (sma_20 - sma_20.shift(5)) / sma_20.shift(5)\n",
        "  bb = BollingerBands(close = df['Close'], window = 20, window_dev = 2)\n",
        "  df['BB_Width'] = bb.bollinger_wband()\n",
        "\n",
        "  df = df.fillna(method = 'ffill').fillna(0)\n",
        "  return df\n",
        "\n",
        "def get_5day_features(tier = 'tier3'):\n",
        "  base = get_feature_columns(tier)\n",
        "  extra = ['Trend_Slope', 'BB_Width', 'SPY_RSI', 'SPY_Trend']\n",
        "  return base + extra\n"
      ],
      "metadata": {
        "id": "C-tqNyKbxaKv"
      },
      "id": "C-tqNyKbxaKv",
      "execution_count": 44,
      "outputs": []
    },
    {
      "metadata": {
        "id": "15f76f99925dc480"
      },
      "cell_type": "markdown",
      "source": [
        "Testing with main and creation of app!"
      ],
      "id": "15f76f99925dc480"
    },
    {
      "cell_type": "code",
      "source": [
        "PREDICTED_HORI = 5\n",
        "CONFIDENCE_THRESHOLD = 0.70\n",
        "\n",
        "print(f\"Targeting: {len(TICKERS)} stocks | Horizon: {PREDICTED_HORI} Days...\")\n",
        "print(f\"Confidence Threshold: {CONFIDENCE_THRESHOLD * 100}%\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# download market context for better predictions\n",
        "spy_context = download_stock_data('SPY', START_DATE, END_DATE)\n",
        "spy_context['SPY_RSI'] = RSIIndicator(close = spy_context['Close'], window = 14).rsi()\n",
        "\n",
        "# Is spy long-term uptrend?\n",
        "spy_context['SPY_Trend'] = (spy_context['Close'] > spy_context['Close'].rolling(200).mean()).astype(int)\n",
        "spy_features = spy_context[['SPY_RSI', 'SPY_Trend']].copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83IEk00twbTJ",
        "outputId": "f9193bc4-2aff-44aa-a91c-f0bec33b06fc"
      },
      "id": "83IEk00twbTJ",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Targeting: 10 stocks | Horizon: 5 Days...\n",
            "Confidence Threshold: 70.0%\n",
            "--------------------------------------------------\n",
            "Downloading SPY data from 2013-01-01 to 2025-05-05...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional helper functions\n",
        "\n",
        "\"\"\"\n",
        "I added this after I made the original model to try to push accuracy up to 60%\n",
        "\"\"\"\n",
        "\n",
        "def get_sniper_features(tier='tier3'):\n",
        "    # We take the standard features and add Market Context\n",
        "    base = get_feature_columns(tier)\n",
        "    return base + ['SPY_RSI', 'SPY_Trend', 'Trend_Slope', 'BB_Width']\n",
        "\n",
        "def add_market_context(df, spy_features):\n",
        "    # Merges the SPY data into the individual stock data\n",
        "    df = df.merge(spy_features, left_index=True, right_index=True, how='left')\n",
        "\n",
        "    # Add Trend Slope\n",
        "    sma_20 = df['Close'].rolling(window=20).mean()\n",
        "    df['Trend_Slope'] = (sma_20 - sma_20.shift(5)) / sma_20.shift(5)\n",
        "\n",
        "    # Add Volatility Squeeze\n",
        "    bb = BollingerBands(close=df['Close'], window=20, window_dev=2)\n",
        "    df['BB_Width'] = bb.bollinger_wband()\n",
        "\n",
        "    return df.fillna(method='ffill').fillna(0)\n",
        "\n",
        "def train_and_evaluate_sniper(df, ticker, tier='tier3'):\n",
        "    # Config\n",
        "    CONFIDENCE_THRESHOLD = 0.60\n",
        "    feature_cols = get_sniper_features(tier)\n",
        "\n",
        "    X = df[feature_cols]\n",
        "    y = df['target']\n",
        "\n",
        "    # Time Series Split (Gap=5 to prevent leakage)\n",
        "    tscv = TimeSeriesSplit(n_splits=N_SPLITS, gap=5)\n",
        "\n",
        "    # The Ensemble\n",
        "    ensemble = VotingClassifier(\n",
        "        estimators=[\n",
        "            ('xgb', xgb.XGBClassifier(max_depth=4, learning_rate=0.03, n_estimators=150, eval_metric='logloss', random_state=42)),\n",
        "            ('rf', RandomForestClassifier(n_estimators=150, max_depth=6, min_samples_leaf=5, random_state=42, n_jobs=-1))\n",
        "        ],\n",
        "        voting='soft'\n",
        "    )\n",
        "\n",
        "    # define pipeline\n",
        "    training_pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()), # scale\n",
        "        ('ensemble', ensemble) # model\n",
        "    ])\n",
        "\n",
        "    # remove the rest of the steps that I completed manually\n",
        "\n",
        "    base_accuracies = []\n",
        "    high_conf_trades = 0\n",
        "    high_conf_wins = 0\n",
        "    feature_importances = []\n",
        "\n",
        "    for train_idx, test_idx in tscv.split(X):\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "        # Fit the pipeline directly...\n",
        "        training_pipeline.fit(X_train, y_train)\n",
        "\n",
        "        # Standard Accuracy\n",
        "        y_pred = training_pipeline.predict(X_test)\n",
        "        base_accuracies.append(accuracy_score(y_test, y_pred))\n",
        "\n",
        "        # Sniper Accuracy (Confidence > 60%)\n",
        "        probs = training_pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        for i, prob in enumerate(probs):\n",
        "            actual = y_test.iloc[i]\n",
        "            if prob > CONFIDENCE_THRESHOLD:\n",
        "                high_conf_trades += 1\n",
        "                if actual == 1: high_conf_wins += 1\n",
        "            elif prob < (1 - CONFIDENCE_THRESHOLD):\n",
        "                high_conf_trades += 1\n",
        "                if actual == 0: high_conf_wins += 1\n",
        "\n",
        "        # Extract Importance from RF part\n",
        "        rf_step = training_pipeline.named_steps['ensemble'].named_estimators_['rf']\n",
        "        feature_importances.append(ensemble.named_estimators_['rf'].feature_importances_)\n",
        "\n",
        "    # Compile Results\n",
        "    avg_base = np.mean(base_accuracies)\n",
        "    sniper_acc = (high_conf_wins / high_conf_trades) if high_conf_trades > 0 else 0.5\n",
        "\n",
        "\n",
        "    filename = f\"{ticker}_sniper_pipeline.pkl\"\n",
        "    joblib.dump(training_pipeline, filename)\n",
        "    print(f\"  > Model saves to {filename}\") # sidenote, but I decided to use this format out of habit because of my C++ course\n",
        "\n",
        "    print(f\"   > Base Acc: {avg_base:.1%} | Sniper Acc: {sniper_acc:.1%} ({high_conf_trades} trades)\")\n",
        "\n",
        "    return {\n",
        "        'ticker': ticker,\n",
        "        'model_type': 'Sniper_Ensemble',\n",
        "        'tier': tier,\n",
        "        'avg_accuracy': avg_base,       # Standard Accuracy\n",
        "        'sniper_accuracy': sniper_acc,  # The High Confidence Accuracy\n",
        "        'trades': high_conf_trades,\n",
        "        'fold_accuracies': base_accuracies, # Needed for save function\n",
        "        'std_accuracy': np.std(base_accuracies), # Needed for save function\n",
        "        'min_accuracy': min(base_accuracies),\n",
        "        'max_accuracy': max(base_accuracies),\n",
        "        'feature_importance': dict(zip(feature_cols, np.mean(feature_importances, axis=0)))\n",
        "    }"
      ],
      "metadata": {
        "id": "NxeUFfz-Bk-D"
      },
      "id": "NxeUFfz-Bk-D",
      "execution_count": 46,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-04T11:57:12.906681Z",
          "start_time": "2025-12-04T11:57:05.028816Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86181052a85e3f5",
        "outputId": "21b66367-e6f1-4878-8aab-0b097f222e0b",
        "collapsed": true
      },
      "cell_type": "code",
      "source": [
        "all_results = []\n",
        "\n",
        "print(\"Starting Sniper Pipeline...\")\n",
        "print(f\"Targeting: {len(TICKERS)} stocks | Horizon: {PREDICTION_HORIZON} days\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Download Market Context\n",
        "print(\"Downloading SPY Context...\")\n",
        "spy_context = download_stock_data('SPY', START_DATE, END_DATE)\n",
        "spy_context['SPY_RSI'] = RSIIndicator(close=spy_context['Close'], window=14).rsi()\n",
        "spy_context['SPY_Trend'] = (spy_context['Close'] > spy_context['Close'].rolling(200).mean()).astype(int)\n",
        "spy_features = spy_context[['SPY_RSI', 'SPY_Trend']].copy()\n",
        "\n",
        "for ticker in TICKERS:\n",
        "    try:\n",
        "        # Download\n",
        "        df = download_stock_data(ticker, START_DATE, END_DATE)\n",
        "\n",
        "        # Check if data is valid\n",
        "        if df is None or len(df) < 200:\n",
        "            print(f\"Skipping {ticker}: Insufficient data.\")\n",
        "            continue\n",
        "\n",
        "        # Add Indicators\n",
        "        try:\n",
        "            df = add_all_indicators(df, tier='tier3')\n",
        "\n",
        "            # Inject Market Context\n",
        "            df = add_market_context(df, spy_features)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error adding indicators for {ticker}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Create Target\n",
        "        df = create_target_var(df, horizon=PREDICTION_HORIZON)\n",
        "        df = df.dropna()\n",
        "\n",
        "        # Train & Evaluate using Sniper Function\n",
        "        print(f\"Training {ticker}...\", end=\"\")\n",
        "        result = train_and_evaluate_sniper(df, ticker, tier='tier3')\n",
        "\n",
        "        if result:\n",
        "            all_results.append(result)\n",
        "            try:\n",
        "                plot_feature_importance(result)\n",
        "            except Exception as e:\n",
        "                print(f\"Could not plot importance for {ticker}: {e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nCRITICAL ERROR processing {ticker}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "# Final Reporting\n",
        "if all_results:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Generating Final Reports...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Summary for Sniper Results\n",
        "    print(f\"\\n{'Ticker':<6} | {'Base Acc':<10} | {'Sniper Acc (>60%)':<20} | {'Trades'}\")\n",
        "    print(\"-\" * 65)\n",
        "    for r in all_results:\n",
        "        print(f\"{r['ticker']:<6} | {r['avg_accuracy']:.1%}     | {r['sniper_accuracy']:.1%}               | {r['trades']}\")\n",
        "\n",
        "    try:\n",
        "        save_detailed_results(all_results)\n",
        "        plot_accuracy_comparison(all_results)\n",
        "        print(\"\\nExecution Complete! Check the './plots' folder for images.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating final reports: {e}\")\n",
        "else:\n",
        "    print(\"\\nNo results were generated. Please check your data connection and try again.\")"
      ],
      "id": "86181052a85e3f5",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Sniper Pipeline...\n",
            "Targeting: 10 stocks | Horizon: 5 days\n",
            "--------------------------------------------------\n",
            "Downloading SPY Context...\n",
            "Downloading SPY data from 2013-01-01 to 2025-05-05...\n",
            "Downloading SPY data from 2013-01-01 to 2025-05-05...\n",
            "Training SPY...  > Model saves to SPY_sniper_pipeline.pkl\n",
            "   > Base Acc: 58.0% | Sniper Acc: 59.0% (1608 trades)\n",
            "Downloading QQQ data from 2013-01-01 to 2025-05-05...\n",
            "Training QQQ...  > Model saves to QQQ_sniper_pipeline.pkl\n",
            "   > Base Acc: 56.8% | Sniper Acc: 58.5% (1616 trades)\n",
            "Downloading AAPL data from 2013-01-01 to 2025-05-05...\n",
            "Training AAPL...  > Model saves to AAPL_sniper_pipeline.pkl\n",
            "   > Base Acc: 54.7% | Sniper Acc: 54.9% (1200 trades)\n",
            "Downloading NVDA data from 2013-01-01 to 2025-05-05...\n",
            "Training NVDA...  > Model saves to NVDA_sniper_pipeline.pkl\n",
            "   > Base Acc: 55.1% | Sniper Acc: 56.2% (1594 trades)\n",
            "Downloading MSFT data from 2013-01-01 to 2025-05-05...\n",
            "Training MSFT...  > Model saves to MSFT_sniper_pipeline.pkl\n",
            "   > Base Acc: 57.0% | Sniper Acc: 58.3% (1246 trades)\n",
            "Downloading JPM data from 2013-01-01 to 2025-05-05...\n",
            "Training JPM...  > Model saves to JPM_sniper_pipeline.pkl\n",
            "   > Base Acc: 52.6% | Sniper Acc: 53.7% (1200 trades)\n",
            "Downloading AMZN data from 2013-01-01 to 2025-05-05...\n",
            "Training AMZN...  > Model saves to AMZN_sniper_pipeline.pkl\n",
            "   > Base Acc: 53.2% | Sniper Acc: 55.4% (1052 trades)\n",
            "Downloading XOM data from 2013-01-01 to 2025-05-05...\n",
            "Training XOM...  > Model saves to XOM_sniper_pipeline.pkl\n",
            "   > Base Acc: 51.0% | Sniper Acc: 51.4% (1135 trades)\n",
            "Downloading BAC data from 2013-01-01 to 2025-05-05...\n",
            "Training BAC...  > Model saves to BAC_sniper_pipeline.pkl\n",
            "   > Base Acc: 51.0% | Sniper Acc: 51.3% (1217 trades)\n",
            "Downloading JNJ data from 2013-01-01 to 2025-05-05...\n",
            "Training JNJ...  > Model saves to JNJ_sniper_pipeline.pkl\n",
            "   > Base Acc: 51.0% | Sniper Acc: 52.1% (1206 trades)\n",
            "\n",
            "==================================================\n",
            "Generating Final Reports...\n",
            "==================================================\n",
            "\n",
            "Ticker | Base Acc   | Sniper Acc (>60%)    | Trades\n",
            "-----------------------------------------------------------------\n",
            "SPY    | 58.0%     | 59.0%               | 1608\n",
            "QQQ    | 56.8%     | 58.5%               | 1616\n",
            "AAPL   | 54.7%     | 54.9%               | 1200\n",
            "NVDA   | 55.1%     | 56.2%               | 1594\n",
            "MSFT   | 57.0%     | 58.3%               | 1246\n",
            "JPM    | 52.6%     | 53.7%               | 1200\n",
            "AMZN   | 53.2%     | 55.4%               | 1052\n",
            "XOM    | 51.0%     | 51.4%               | 1135\n",
            "BAC    | 51.0%     | 51.3%               | 1217\n",
            "JNJ    | 51.0%     | 52.1%               | 1206\n",
            "\n",
            "Detailed results saved to: detailed_results.csv\n",
            "\n",
            "Execution Complete! Check the './plots' folder for images.\n"
          ]
        }
      ],
      "execution_count": 47
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yVsxOFoFtU-I"
      },
      "id": "yVsxOFoFtU-I",
      "execution_count": 47,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}