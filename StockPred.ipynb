{
  "cells": [
    {
      "metadata": {
        "id": "8a77807f92f26ee"
      },
      "cell_type": "markdown",
      "source": [
        "Stock Prediction Project"
      ],
      "id": "8a77807f92f26ee"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-04T11:53:17.298660Z",
          "start_time": "2025-12-04T11:53:17.294910Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbc121e30a2defb3",
        "outputId": "c6efeab4-b8bb-45e7-82ec-8593b86785b4"
      },
      "cell_type": "code",
      "source": [
        "#Import libs\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pkg_resources import non_empty_lines\n",
        "\n",
        "#indicators\n",
        "from ta.momentum import RSIIndicator, StochasticOscillator, ROCIndicator, WilliamsRIndicator\n",
        "from ta.trend import MACD, ADXIndicator, EMAIndicator, CCIIndicator, AroonIndicator\n",
        "from ta.volatility import BollingerBands, AverageTrueRange\n",
        "from ta.volume import OnBalanceVolumeIndicator, MFIIndicator\n",
        "\n",
        "#ML stuffs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "# Visual + Warn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "# Finance data\n",
        "import yfinance as yf\n",
        "from datetime import datetime\n",
        "\n",
        "# Streamlit\n",
        "import streamlit as st\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ],
      "id": "fbc121e30a2defb3",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-04T11:53:18.073522Z",
          "start_time": "2025-12-04T11:53:18.070696Z"
        },
        "id": "260f66356d827ef0"
      },
      "cell_type": "code",
      "source": [
        "# Configs from necessary research\n",
        "\n",
        "TICKERS = [ # I decided to remove tesla since its movement is usually based on the CEO and news...\n",
        "    'SPY', # INDEX\n",
        "    'QQQ',\n",
        "    'AAPL', # Heavy hitter for S&P\n",
        "    'NVDA',\n",
        "    'MSFT', # Heavy hitter for S&P\n",
        "    'JPM',\n",
        "    'AMZN', # Heavy hitter for S&P\n",
        "    'XOM',\n",
        "    'BAC',\n",
        "    'JNJ'\n",
        "]\n",
        "\n",
        "# For our start dates, we are going to go back 10 years.\n",
        "\n",
        "START_DATE = '2015-01-01'\n",
        "END_DATE = '2025-01-01'\n",
        "\n",
        "# 5 day forward predictions\n",
        "PREDICTION_HORIZON = 5\n",
        "\n",
        "# Cross-Val\n",
        "N_SPLITS = 5\n",
        "\n",
        "# Gap\n",
        "GAP = 5\n",
        "\n",
        "\n",
        "# Configs for graphs\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)"
      ],
      "id": "260f66356d827ef0",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-04T11:56:58.592765Z",
          "start_time": "2025-12-04T11:56:58.578473Z"
        },
        "id": "7f9bd395935dade"
      },
      "cell_type": "code",
      "source": [
        "# Helper functions\n",
        "\n",
        "def download_stock_data(ticker, start, end):\n",
        "    try:\n",
        "        print(f\"Downloading {ticker} data from {start} to {end}...\")\n",
        "        # DEBUG: Added multi_level_index=False to prevent 2D shape errors\n",
        "        df = yf.download(ticker, start=start, end=end, auto_adjust=True, progress=False, multi_level_index=False)\n",
        "\n",
        "        if df.empty:\n",
        "            print(f\"No data available for {ticker}...\")\n",
        "            return None\n",
        "\n",
        "        # Flatten columns if yfinance returns a MultiIndex\n",
        "        if isinstance(df.columns, pd.MultiIndex):\n",
        "            df.columns = df.columns.get_level_values(0)\n",
        "\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {ticker}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def add_tier1_indicators(df): # these will provide 80% of our predictive power\n",
        "    # RSI is what 90% of traders use. This stands for relative strength index\n",
        "    df['rsi'] = RSIIndicator(close = df['Close'], window = 14).rsi()\n",
        "\n",
        "    # MACD is the next ind we plan to implement.\n",
        "    macd = MACD(close=df['Close'], window_slow=26, window_fast=12, window_sign=9)\n",
        "    df['macd_line'] = macd.macd()\n",
        "    df['macd_signal'] = macd.macd_signal()\n",
        "    df['macd_hist'] = macd.macd_diff()\n",
        "\n",
        "    # Bollinger Bands\n",
        "    bb = BollingerBands(close=df['Close'], window=20, window_dev = 2)\n",
        "    df['bb_width'] = bb.bollinger_wband()\n",
        "    df['bb_percent'] = bb.bollinger_pband()\n",
        "\n",
        "    # ADX (Average Directional Index)\n",
        "    adx = ADXIndicator(high=df['High'], low=df['Low'], close=df['Close'], window=14)\n",
        "    df['adx'] = adx.adx()\n",
        "    df['adx_pos'] = adx.adx_pos()\n",
        "    df['adx_neg'] = adx.adx_neg()\n",
        "    df['adx_diff'] = df['adx_pos'] - df['adx_neg'] # this is our directional component for better analysis\n",
        "\n",
        "    # ATR (Average True Range)\n",
        "    atr = AverageTrueRange(high=df['High'], low=df['Low'], close=df['Close'], window=14)\n",
        "    df['atr_norm'] = atr.average_true_range() / df['Close']\n",
        "\n",
        "    # This one must be normalized ^^^\n",
        "    # These should be about 10 features\n",
        "\n",
        "\n",
        "    return df\n",
        "\n",
        "def add_tier2_indicators(df):\n",
        "    # These are recommended indicators that will increase accuracy and add diversity to the set\n",
        "    # This should add around 8 features\n",
        "\n",
        "    # Stoch Osci\n",
        "    stoch = StochasticOscillator(high=df['High'], low=df['Low'], close=df['Close'], window=14, smooth_window=3)\n",
        "    df['stoch_k'] = stoch.stoch()\n",
        "    df['stoch_d'] = stoch.stoch_signal()\n",
        "\n",
        "\n",
        "    # OBV (on balance volume) - This is should use rate of change and not absolute\n",
        "    obv = OnBalanceVolumeIndicator(close = df['Close'], volume = df['Volume'])\n",
        "    df['obv_roc'] = obv.on_balance_volume().pct_change(5)\n",
        "\n",
        "    # Williams %R\n",
        "    df['williams_r'] = WilliamsRIndicator(high=df['High'], low=df['Low'], close = df['Close'], lbp = 14).williams_r()\n",
        "\n",
        "    # EMA\n",
        "    ema_12 = EMAIndicator(close = df['Close'], window=12).ema_indicator()\n",
        "    ema_26 = EMAIndicator(close = df['Close'], window=26).ema_indicator()\n",
        "    df['ema_ratio'] = ema_12 / ema_26\n",
        "\n",
        "    # ROC (Rate of Change)\n",
        "    df['roc_5'] = ROCIndicator(close = df['Close'], window = 5).roc()\n",
        "    df['roc_10'] = ROCIndicator(close = df['Close'], window = 10).roc()\n",
        "    return df\n",
        "\n",
        "def add_tier3_indicators(df):\n",
        "    # these are optional extra that will add 6 additional features to help improve acc\n",
        "\n",
        "    # CCI Comod chan index\n",
        "    df['cci'] = CCIIndicator(high = df['High'], low = df['Low'], close = df['Close'], window = 20).cci()\n",
        "\n",
        "    # MFI money flow index\n",
        "    df['mfi'] = MFIIndicator(high = df['High'], low = df['Low'], close = df['Close'], volume = df['Volume'], window = 14).money_flow_index()\n",
        "\n",
        "    # Aroon Indic\n",
        "    aroon = AroonIndicator(high=df['High'], low=df['Low'], window=25)\n",
        "    df['aroon_indicator'] = aroon.aroon_indicator()\n",
        "\n",
        "    return df\n",
        "\n",
        "def add_all_indicators(df, tier = 'tier2'):\n",
        "    # With this function we should make sure to add technical indicators based on the tier that is selected\n",
        "\n",
        "    '''\n",
        "    :param tier: tier1 (10 feat)\n",
        "    :param tier: tier2 (18 feat)\n",
        "    :param tier: tier3 (24 feat)\n",
        "    '''\n",
        "\n",
        "    df = add_tier1_indicators(df)\n",
        "\n",
        "    if tier in ['tier2', 'tier3']:\n",
        "        df = add_tier2_indicators(df)\n",
        "    if tier == 'tier3':\n",
        "        df = add_tier3_indicators(df)\n",
        "\n",
        "    return df\n",
        "\n",
        "def create_target_var(df, horizon=5):\n",
        "    # We need to have a function that creates a binary target. If price goes up compared to the horizon days, return 1.\n",
        "\n",
        "    df['target'] = (df['Close'].shift(-horizon) > df['Close']).astype(int)\n",
        "    return df\n",
        "\n",
        "def get_feature_columns(tier = 'tier2'):\n",
        "    # Return feature column names based on the selected tier\n",
        "\n",
        "    tier1_features = [\n",
        "        'rsi',\n",
        "        'macd_hist',\n",
        "        'bb_width',\n",
        "        'bb_percent',\n",
        "        'adx',\n",
        "        'adx_diff',\n",
        "        'atr_norm'\n",
        "    ]\n",
        "\n",
        "    tier2_features = [\n",
        "        'stoch_k',\n",
        "        'obv_roc',\n",
        "        'williams_r',\n",
        "        'ema_ratio',\n",
        "        'roc_5',\n",
        "        'roc_10'\n",
        "    ]\n",
        "\n",
        "    tier3_features = [\n",
        "        'cci',\n",
        "        'mfi',\n",
        "        'aroon_indicator'\n",
        "    ]\n",
        "\n",
        "    if tier == 'tier1':\n",
        "        return tier1_features\n",
        "    elif tier == 'tier2':\n",
        "        return tier1_features + tier2_features\n",
        "    else:\n",
        "        return tier1_features + tier2_features + tier3_features\n",
        "\n",
        "def check_feature_correlation(df, feature_cols, threshold=0.9):\n",
        "    \"\"\"\n",
        "    With this function we need to check for highly correlated features and recommend removal. From my research document, I have found that correlation >9.9 should be avoided and removed.\n",
        "    \"\"\"\n",
        "\n",
        "    corr_matrix = df[feature_cols].corr().abs()\n",
        "\n",
        "    high_corr_pairs = []\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i+1, len(corr_matrix.columns)):\n",
        "            if corr_matrix.iloc[i, j] > threshold:\n",
        "                high_corr_pairs.append({\n",
        "                    'feature1':corr_matrix.columns[i],\n",
        "                    'feature2':corr_matrix.columns[j],\n",
        "                    'correlation':corr_matrix.iloc[i, j]\n",
        "                })\n",
        "\n",
        "    if high_corr_pairs:\n",
        "        print(f\"\\n High correlation pairs found (>{threshold})\")\n",
        "        for pair in high_corr_pairs:\n",
        "            print(f\" - {pair['feature1']} <-> {pair['feature2']}: {pair['correlation']:.3f}\")\n",
        "    else:\n",
        "        print(f\"\\n No highly correlated features (>{threshold})\")\n",
        "    return high_corr_pairs\n",
        "\n",
        "def train_xgboost_model(X_train, X_test, y_train, y_test, verbose=False):\n",
        "    \"\"\"\n",
        "    Train with XGBoost with reg params\n",
        "    \"\"\"\n",
        "\n",
        "    model = xgb.XGBClassifier(\n",
        "        max_depth=6,\n",
        "        min_child_weight=3,\n",
        "        subsample=0.7,\n",
        "        colsample_bytree=0.7,\n",
        "        learning_rate=0.05,\n",
        "        n_estimators=100,\n",
        "        reg_lambda=1.0,\n",
        "        random_state=42,\n",
        "        eval_metric='logloss',\n",
        "        use_label_encoder=False,\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train, verbose=verbose)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    return model, y_pred, accuracy\n",
        "\n",
        "def train_random_forest_model(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Train with random forest with reg params\n",
        "    :param X_train:\n",
        "    :param X_test:\n",
        "    :param y_train:\n",
        "    :param y_test:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=10,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        max_features='sqrt',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    return model, y_pred, accuracy\n",
        "\n",
        "def train_and_evaluate(df, ticker, tier='tier2', model_type='xgboost'):\n",
        "    # Train model with time-series cross val and comp eval\n",
        "    feature_cols = get_feature_columns(tier)\n",
        "    df_clean = df.dropna()\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Training {model_type.upper()} model for {ticker} (Tier: {tier})\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Total samples: {len(df_clean):,}\")\n",
        "    print(f\"Features: {len(feature_cols)}\")\n",
        "    print(f\"Date range: {df_clean.index[0].date()} to {df_clean.index[-1].date()}\")\n",
        "\n",
        "    # Check for empty data before proceeding\n",
        "    if len(df_clean) < 100:\n",
        "        print(\"Not enough data to train. Skipping.\")\n",
        "        return None\n",
        "\n",
        "    class_balance = df_clean['target'].value_counts()\n",
        "    print(f\"\\nClass balance:\")\n",
        "    try:\n",
        "        print(f\"  Down (0): {class_balance[0]:,} ({class_balance[0]/len(df_clean)*100:.1f}%)\")\n",
        "        print(f\"  Up (1): {class_balance[1]:,} ({class_balance[1]/len(df_clean)*100:.1f}%)\")\n",
        "    except KeyError:\n",
        "        print(f\"  Class balance issue: {class_balance}\")\n",
        "\n",
        "    check_feature_correlation(df_clean, feature_cols)\n",
        "    tscv = TimeSeriesSplit(n_splits=N_SPLITS, gap=GAP)\n",
        "\n",
        "    fold_accuracies = []\n",
        "    all_predictions = []\n",
        "    all_actual = [] # This lists needs to store VALUES, not DataFrames!!!\n",
        "    feature_importances = []\n",
        "\n",
        "    print(f\"\\nCross-validation with {N_SPLITS} folds (gap={GAP} days):\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(tscv.split(df_clean), 1):\n",
        "        X_train = df_clean.iloc[train_idx][feature_cols]\n",
        "        X_test = df_clean.iloc[test_idx][feature_cols]\n",
        "        y_train = df_clean.iloc[train_idx][['target']]\n",
        "        y_test = df_clean.iloc[test_idx][['target']]\n",
        "\n",
        "        train_dates = df_clean.iloc[train_idx].index\n",
        "        test_dates = df_clean.iloc[test_idx].index\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        if model_type == 'xgboost':\n",
        "            model, y_pred, accuracy = train_xgboost_model(\n",
        "                X_train_scaled, X_test_scaled, y_train, y_test\n",
        "            )\n",
        "        else:\n",
        "            model, y_pred, accuracy = train_random_forest_model(\n",
        "                X_train_scaled, X_test_scaled, y_train, y_test\n",
        "            )\n",
        "\n",
        "        fold_accuracies.append(accuracy)\n",
        "        all_predictions.extend(y_pred)\n",
        "\n",
        "\n",
        "        all_actual.extend(y_test['target'].values)\n",
        "\n",
        "        feature_importances.append(model.feature_importances_)\n",
        "\n",
        "        print(f\"Fold {fold}: {accuracy:.4f} ({accuracy*100:.2f}%) | \"\n",
        "              f\"Train: {train_dates[0].date()} to {train_dates[-1].date()} | \"\n",
        "              f\"Test: {test_dates[0].date()} to {test_dates[-1].date()}\")\n",
        "\n",
        "    avg_accuracy = np.mean(fold_accuracies)\n",
        "    std_accuracy = np.std(fold_accuracies)\n",
        "    min_accuracy = min(fold_accuracies)\n",
        "    max_accuracy = max(fold_accuracies)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Overall Results for {ticker}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Average Accuracy: {avg_accuracy:.4f} ({avg_accuracy*100:.2f}%)\")\n",
        "\n",
        "    if avg_accuracy >= 0.60:\n",
        "        print(f\"Target Met: Achieved 60%+ accuracy!\")\n",
        "    else:\n",
        "        gap = (0.60 - avg_accuracy) * 100\n",
        "        print(f\"Target Not Met: {gap:.2f}% below 60% target\")\n",
        "\n",
        "    # Generate Report\n",
        "    # Ensure inputs are numpy arrays or lists of equal length\n",
        "    print(f\"\\nClassification Report:\")\n",
        "    print(classification_report(all_actual, all_predictions, target_names=['Down', 'Up'], digits=4))\n",
        "\n",
        "    cm = confusion_matrix(all_actual, all_predictions)\n",
        "\n",
        "    # Final Model Training\n",
        "    X_all = df_clean[feature_cols]\n",
        "    y_all = df_clean['target']\n",
        "    scaler_final = StandardScaler()\n",
        "    X_all_scaled = scaler_final.fit_transform(X_all)\n",
        "\n",
        "    if model_type == 'xgboost':\n",
        "        final_model = xgb.XGBClassifier(\n",
        "            max_depth=6,\n",
        "            min_child_weight=3,\n",
        "            subsample=0.7,\n",
        "            colsample_bytree=0.7,\n",
        "            learning_rate=0.05,\n",
        "            n_estimators=100,\n",
        "            reg_lambda=1.0,\n",
        "            random_state=42,\n",
        "            eval_metric='logloss',\n",
        "            use_label_encoder=False\n",
        "        )\n",
        "    else:\n",
        "        final_model = RandomForestClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=10,\n",
        "            min_samples_split=5,\n",
        "            min_samples_leaf=2,\n",
        "            max_features='sqrt',\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "    final_model.fit(X_all_scaled, y_all)\n",
        "    avg_feature_importance = np.mean(feature_importances, axis=0)\n",
        "\n",
        "    return {\n",
        "        'ticker': ticker,\n",
        "        'model_type': model_type,\n",
        "        'tier': tier,\n",
        "        'model': final_model,\n",
        "        'avg_accuracy': avg_accuracy,\n",
        "        'std_accuracy': std_accuracy,\n",
        "        'min_accuracy': min_accuracy,\n",
        "        'max_accuracy': max_accuracy,\n",
        "        'fold_accuracies': fold_accuracies,\n",
        "        'feature_cols': feature_cols,\n",
        "        'feature_importance': dict(zip(feature_cols, avg_feature_importance)),\n",
        "        'confusion_matrix': cm\n",
        "    }"
      ],
      "id": "7f9bd395935dade",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-04T11:56:59.875429Z",
          "start_time": "2025-12-04T11:56:59.865695Z"
        },
        "id": "711e8cf8692650bd"
      },
      "cell_type": "code",
      "source": [
        "def plot_feature_importance(results, save_dir='./plots'):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    importance_df = pd.DataFrame([\n",
        "        {'feature' : k, 'importance' : v}\n",
        "        for k, v in results['feature_importance'].items()\n",
        "    ]).sort_values('importance', ascending=False)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    colors = sns.color_palette(\"viridis\", len(importance_df))\n",
        "    sns.barplot(data=importance_df, y='feature', x='importance', palette=colors)\n",
        "    plt.title(f\"Feature Importance - {results['ticker']} ({results['model_type'].upper()})\\n\"\n",
        "              f\"Accuracy: {results['avg_accuracy']*100:.2f}%\", fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Importance Score', fontsize=12)\n",
        "    plt.ylabel('Feature', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    filename = f\"{save_dir}/feature_importance_{results['ticker']}_{results['model_type']}.png\"\n",
        "    plt.savefig(filename, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return filename\n",
        "\n",
        "def plot_accuracy_comparison(all_results, save_dir='./plots'):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    comparison_data = []\n",
        "    for result in all_results:\n",
        "        comparison_data.append({\n",
        "            'Ticker': result['ticker'],\n",
        "            'Accuracy': result['avg_accuracy'] * 100,\n",
        "            'Model': result['model_type'].upper()\n",
        "        })\n",
        "\n",
        "    df_comp = pd.DataFrame(comparison_data)\n",
        "\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    ax = sns.barplot(data=df_comp, x='Ticker', y='Accuracy', hue='Model', palette='Set2')\n",
        "\n",
        "    plt.axhline(y=60, color='red', linestyle='--', linewidth=2, label='60% Target')\n",
        "\n",
        "    for container in ax.containers:\n",
        "        ax.bar_label(container, fmt='%.1f%%', padding=3)\n",
        "\n",
        "    plt.title('Model Accuracy Comparison Across Stocks\\n5-Day Price Direction Prediction',\n",
        "              fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Stock Ticker', fontsize=12)\n",
        "    plt.ylabel('Accuracy (%)', fontsize=12)\n",
        "    plt.legend(title='Model Type', fontsize=10)\n",
        "    plt.ylim(40, max(df_comp['Accuracy'].max() + 5, 70))\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    filename = f\"{save_dir}/accuracy_comparison.png\"\n",
        "    plt.savefig(filename, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return filename\n",
        "\n",
        "def save_detailed_results(all_results, filename = 'detailed_results.csv'):\n",
        "    detailed_data = []\n",
        "\n",
        "    for result in all_results:\n",
        "        for fold_num, fold_acc in enumerate(result['fold_accuracies'], 1):\n",
        "            detailed_data.append({\n",
        "                'Ticker': result['ticker'],\n",
        "                'Model': result['model_type'],\n",
        "                'Tier': result['tier'],\n",
        "                'Fold': fold_num,\n",
        "                'Accuracy': fold_acc * 100,\n",
        "                'Average_Accuracy': result['avg_accuracy'] * 100,\n",
        "                'Std_Dev': result['std_accuracy'] * 100,\n",
        "                'Min_Accuracy': result['min_accuracy'] * 100,\n",
        "                'Max_Accuracy': result['max_accuracy'] * 100\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(detailed_data)\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"\\nDetailed results saved to: {filename}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def print_final_summary(all_results):\n",
        "    \"\"\"Print comprehensive final summary.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"FINAL SUMMARY - ALL STOCKS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    summary_data = []\n",
        "    for r in all_results:\n",
        "        summary_data.append({\n",
        "            'Ticker': r['ticker'],\n",
        "            'Model': r['model_type'].upper(),\n",
        "            'Tier': r['tier'].upper(),\n",
        "            'Avg Acc': f\"{r['avg_accuracy']*100:.2f}%\",\n",
        "            'Std Dev': f\"{r['std_accuracy']*100:.2f}%\",\n",
        "            'Min': f\"{r['min_accuracy']*100:.2f}%\",\n",
        "            'Max': f\"{r['max_accuracy']*100:.2f}%\",\n",
        "            'Target Met': '✓' if r['avg_accuracy'] >= 0.60 else '✗'\n",
        "        })\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    print(summary_df.to_string(index=False))\n",
        "\n",
        "    overall_avg = np.mean([r['avg_accuracy'] for r in all_results])\n",
        "    overall_std = np.std([r['avg_accuracy'] for r in all_results])\n",
        "    stocks_above_60 = sum(1 for r in all_results if r['avg_accuracy'] >= 0.60)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Overall Statistics\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Average Accuracy Across All Stocks: {overall_avg*100:.2f}% (±{overall_std*100:.2f}%)\")\n",
        "    print(f\"Stocks Meeting 60% Target: {stocks_above_60}/{len(all_results)}\")\n",
        "    print(f\"Best Performing Stock: {max(all_results, key=lambda x: x['avg_accuracy'])['ticker']} \"\n",
        "          f\"({max(r['avg_accuracy'] for r in all_results)*100:.2f}%)\")\n",
        "    print(f\"Worst Performing Stock: {min(all_results, key=lambda x: x['avg_accuracy'])['ticker']} \"\n",
        "          f\"({min(r['avg_accuracy'] for r in all_results)*100:.2f}%)\")\n",
        "\n",
        "    if overall_avg >= 0.60:\n",
        "        print(f\"\\nSuccess: Overall average EXCEEDS 60% target!\")\n",
        "    else:\n",
        "        gap = (0.60 - overall_avg) * 100\n",
        "        print(f\"\\nOverall average is {gap:.2f}% below 60% target\")\n",
        "        print(f\"Recommendations:\")\n",
        "        print(f\"   - Try tier3 indicators for more features\")\n",
        "        print(f\"   - Test Random Forest model as alternative\")\n",
        "        print(f\"   - Consider ensemble methods (combine XGBoost + RF)\")\n",
        "\n",
        "    return summary_df\n"
      ],
      "id": "711e8cf8692650bd",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "I want to use StreamLit to create an interactive dashboard like page for the charts"
      ],
      "metadata": {
        "id": "2KJPJFWuRIV4"
      },
      "id": "2KJPJFWuRIV4"
    },
    {
      "cell_type": "code",
      "source": [
        "def render_header():\n",
        "  st.title(\"Machine Learning Stock Predictor Dashboard\")\n",
        "  st.markdown(\"\"\"\n",
        "  This app is designed to predict whether a stock will close **higher** or **lower** in 5 days based on technical indicators such as RSI, MACD, Bollinger Bands, and more.\"\"\")\n",
        "\n",
        "def render_sidebar():\n",
        "  st.sidebar.header(\"Configuration\")\n",
        "  ticker = st.sidebar.text_input(\"Enter Stock Ticker\", value=\"SPY\").upper()\n",
        "  start_date = st.sidebar.date_input(\"Start Date\", value=pd.to_datetime(\"2015-01-01\"))\n",
        "  end_date = st.sidebar.date_input(\"End Date\", value=pd.to_datetime(\"2025-01-01\"))\n",
        "  return ticker, start_date, end_date\n",
        "\n",
        "def display_metrics(accuracy, last_date):\n",
        "  col1, col2 = st.columns(2)\n",
        "  with col1:\n",
        "    st.metric(\"Model Accuracy\", f\"{accuracy*100:.2f}%\")\n",
        "  with col2:\n",
        "    st.metric(\"Last Date Point\", str(last_date.date()))\n",
        "\n",
        "  if accuracy >= 0.60:\n",
        "    st.success(\"Target Accuracy has been met! (60%+)\")\n",
        "  else:\n",
        "    st.warning(f\"Target Not Met ({accuracy*100:.2f}%)\")\n",
        "\n",
        "def plot_importance(model, feature_cols):\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_cols,\n",
        "        'Importance': model.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    sns.barplot(data=importance_df, x='Importance', y='Feature', palette='viridis', ax=ax)\n",
        "    ax.set_title(\"Feature Importance\")\n",
        "    st.pyplot(fig)\n",
        "\n"
      ],
      "metadata": {
        "id": "tKLG4sdxRH8J"
      },
      "id": "tKLG4sdxRH8J",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "15f76f99925dc480"
      },
      "cell_type": "markdown",
      "source": [
        "Testing with main and creation of app!"
      ],
      "id": "15f76f99925dc480"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-04T11:57:12.906681Z",
          "start_time": "2025-12-04T11:57:05.028816Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86181052a85e3f5",
        "outputId": "7f8d106a-14e1-452a-8917-611d299e0af4"
      },
      "cell_type": "code",
      "source": [
        "all_results = []\n",
        "\n",
        "print(\"Starting ML Pipeline Execution...\")\n",
        "print(f\"Targeting: {len(TICKERS)} stocks | Horizon: {PREDICTION_HORIZON} days\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for ticker in TICKERS:\n",
        "    try:\n",
        "        # Download\n",
        "        df = download_stock_data(ticker, START_DATE, END_DATE)\n",
        "\n",
        "        # Check if data is valid\n",
        "        if df is None or len(df) < 200:\n",
        "            print(f\"Skipping {ticker}: Insufficient data.\")\n",
        "            continue\n",
        "\n",
        "        # Add Indicators\n",
        "        try:\n",
        "            df = add_all_indicators(df, tier='tier3')\n",
        "        except Exception as e:\n",
        "            print(f\"Error adding indicators for {ticker}: {e}\")\n",
        "            continue\n",
        "        # Create Target\n",
        "        df = create_target_var(df, horizon=PREDICTION_HORIZON)\n",
        "        # Train & Evaluate\n",
        "        result = train_and_evaluate(df, ticker, tier='tier3', model_type='xgboost')\n",
        "\n",
        "        if result:\n",
        "            all_results.append(result)\n",
        "\n",
        "            try:\n",
        "                plot_feature_importance(result)\n",
        "            except Exception as e:\n",
        "                print(f\"Could not plot importance for {ticker}: {e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"CRITICAL ERROR processing {ticker}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "\n",
        "if all_results:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Generating Final Reports...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    try:\n",
        "        save_detailed_results(all_results)\n",
        "        plot_accuracy_comparison(all_results)\n",
        "        print_final_summary(all_results)\n",
        "        print(\"\\nExecution Complete! Check the './plots' folder for images.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating final reports: {e}\")\n",
        "else:\n",
        "    print(\"\\nNo results were generated. Please check your data connection and try again.\")"
      ],
      "id": "86181052a85e3f5",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting ML Pipeline Execution...\n",
            "Targeting: 10 stocks | Horizon: 5 days\n",
            "--------------------------------------------------\n",
            "Downloading SPY data from 2015-01-01 to 2025-01-01...\n",
            "\n",
            "======================================================================\n",
            "Training XGBOOST model for SPY (Tier: tier3)\n",
            "======================================================================\n",
            "Total samples: 2,483\n",
            "Features: 16\n",
            "Date range: 2015-02-20 to 2024-12-31\n",
            "\n",
            "Class balance:\n",
            "  Down (0): 961 (38.7%)\n",
            "  Up (1): 1,522 (61.3%)\n",
            "\n",
            " High correlation pairs found (>0.9)\n",
            " - rsi <-> bb_percent: 0.906\n",
            " - rsi <-> adx_diff: 0.943\n",
            " - bb_percent <-> stoch_k: 0.938\n",
            " - bb_percent <-> williams_r: 0.938\n",
            " - bb_percent <-> cci: 0.985\n",
            " - stoch_k <-> williams_r: 1.000\n",
            " - stoch_k <-> cci: 0.903\n",
            " - williams_r <-> cci: 0.903\n",
            "\n",
            "Cross-validation with 5 folds (gap=5 days):\n",
            "----------------------------------------------------------------------\n",
            "Fold 1: 0.4625 (46.25%) | Train: 2015-02-20 to 2016-10-07 | Test: 2016-10-17 to 2018-06-07\n",
            "Fold 2: 0.5351 (53.51%) | Train: 2015-02-20 to 2018-05-31 | Test: 2018-06-08 to 2020-01-29\n",
            "Fold 3: 0.5763 (57.63%) | Train: 2015-02-20 to 2020-01-22 | Test: 2020-01-30 to 2021-09-17\n",
            "Fold 4: 0.5109 (51.09%) | Train: 2015-02-20 to 2021-09-10 | Test: 2021-09-20 to 2023-05-10\n",
            "Fold 5: 0.5448 (54.48%) | Train: 2015-02-20 to 2023-05-03 | Test: 2023-05-11 to 2024-12-31\n",
            "\n",
            "======================================================================\n",
            "Overall Results for SPY\n",
            "======================================================================\n",
            "Average Accuracy: 0.5259 (52.59%)\n",
            "Target Not Met: 7.41% below 60% target\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Down     0.3417    0.2789    0.3071       778\n",
            "          Up     0.6077    0.6752    0.6397      1287\n",
            "\n",
            "    accuracy                         0.5259      2065\n",
            "   macro avg     0.4747    0.4771    0.4734      2065\n",
            "weighted avg     0.5075    0.5259    0.5144      2065\n",
            "\n",
            "Downloading QQQ data from 2015-01-01 to 2025-01-01...\n",
            "\n",
            "======================================================================\n",
            "Training XGBOOST model for QQQ (Tier: tier3)\n",
            "======================================================================\n",
            "Total samples: 2,483\n",
            "Features: 16\n",
            "Date range: 2015-02-20 to 2024-12-31\n",
            "\n",
            "Class balance:\n",
            "  Down (0): 985 (39.7%)\n",
            "  Up (1): 1,498 (60.3%)\n",
            "\n",
            " High correlation pairs found (>0.9)\n",
            " - rsi <-> bb_percent: 0.904\n",
            " - rsi <-> adx_diff: 0.938\n",
            " - bb_percent <-> stoch_k: 0.937\n",
            " - bb_percent <-> williams_r: 0.937\n",
            " - bb_percent <-> cci: 0.986\n",
            " - stoch_k <-> williams_r: 1.000\n",
            " - stoch_k <-> cci: 0.904\n",
            " - williams_r <-> cci: 0.904\n",
            "\n",
            "Cross-validation with 5 folds (gap=5 days):\n",
            "----------------------------------------------------------------------\n",
            "Fold 1: 0.5133 (51.33%) | Train: 2015-02-20 to 2016-10-07 | Test: 2016-10-17 to 2018-06-07\n",
            "Fold 2: 0.5400 (54.00%) | Train: 2015-02-20 to 2018-05-31 | Test: 2018-06-08 to 2020-01-29\n",
            "Fold 3: 0.5738 (57.38%) | Train: 2015-02-20 to 2020-01-22 | Test: 2020-01-30 to 2021-09-17\n",
            "Fold 4: 0.5230 (52.30%) | Train: 2015-02-20 to 2021-09-10 | Test: 2021-09-20 to 2023-05-10\n",
            "Fold 5: 0.5908 (59.08%) | Train: 2015-02-20 to 2023-05-03 | Test: 2023-05-11 to 2024-12-31\n",
            "\n",
            "======================================================================\n",
            "Overall Results for QQQ\n",
            "======================================================================\n",
            "Average Accuracy: 0.5482 (54.82%)\n",
            "Target Not Met: 5.18% below 60% target\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Down     0.3975    0.3109    0.3489       804\n",
            "          Up     0.6142    0.6994    0.6541      1261\n",
            "\n",
            "    accuracy                         0.5482      2065\n",
            "   macro avg     0.5058    0.5052    0.5015      2065\n",
            "weighted avg     0.5298    0.5482    0.5353      2065\n",
            "\n",
            "Downloading AAPL data from 2015-01-01 to 2025-01-01...\n",
            "\n",
            "======================================================================\n",
            "Training XGBOOST model for AAPL (Tier: tier3)\n",
            "======================================================================\n",
            "Total samples: 2,483\n",
            "Features: 16\n",
            "Date range: 2015-02-20 to 2024-12-31\n",
            "\n",
            "Class balance:\n",
            "  Down (0): 1,045 (42.1%)\n",
            "  Up (1): 1,438 (57.9%)\n",
            "\n",
            " High correlation pairs found (>0.9)\n",
            " - rsi <-> adx_diff: 0.955\n",
            " - bb_percent <-> stoch_k: 0.923\n",
            " - bb_percent <-> williams_r: 0.923\n",
            " - bb_percent <-> cci: 0.987\n",
            " - stoch_k <-> williams_r: 1.000\n",
            "\n",
            "Cross-validation with 5 folds (gap=5 days):\n",
            "----------------------------------------------------------------------\n",
            "Fold 1: 0.5085 (50.85%) | Train: 2015-02-20 to 2016-10-07 | Test: 2016-10-17 to 2018-06-07\n",
            "Fold 2: 0.4237 (42.37%) | Train: 2015-02-20 to 2018-05-31 | Test: 2018-06-08 to 2020-01-29\n",
            "Fold 3: 0.5811 (58.11%) | Train: 2015-02-20 to 2020-01-22 | Test: 2020-01-30 to 2021-09-17\n",
            "Fold 4: 0.5400 (54.00%) | Train: 2015-02-20 to 2021-09-10 | Test: 2021-09-20 to 2023-05-10\n",
            "Fold 5: 0.5109 (51.09%) | Train: 2015-02-20 to 2023-05-03 | Test: 2023-05-11 to 2024-12-31\n",
            "\n",
            "======================================================================\n",
            "Overall Results for AAPL\n",
            "======================================================================\n",
            "Average Accuracy: 0.5128 (51.28%)\n",
            "Target Not Met: 8.72% below 60% target\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Down     0.3946    0.3890    0.3918       833\n",
            "          Up     0.5908    0.5966    0.5937      1232\n",
            "\n",
            "    accuracy                         0.5128      2065\n",
            "   macro avg     0.4927    0.4928    0.4927      2065\n",
            "weighted avg     0.5117    0.5128    0.5122      2065\n",
            "\n",
            "Downloading NVDA data from 2015-01-01 to 2025-01-01...\n",
            "\n",
            "======================================================================\n",
            "Training XGBOOST model for NVDA (Tier: tier3)\n",
            "======================================================================\n",
            "Total samples: 2,483\n",
            "Features: 16\n",
            "Date range: 2015-02-20 to 2024-12-31\n",
            "\n",
            "Class balance:\n",
            "  Down (0): 1,001 (40.3%)\n",
            "  Up (1): 1,482 (59.7%)\n",
            "\n",
            " High correlation pairs found (>0.9)\n",
            " - rsi <-> adx_diff: 0.954\n",
            " - bb_percent <-> stoch_k: 0.924\n",
            " - bb_percent <-> williams_r: 0.924\n",
            " - bb_percent <-> cci: 0.984\n",
            " - stoch_k <-> williams_r: 1.000\n",
            "\n",
            "Cross-validation with 5 folds (gap=5 days):\n",
            "----------------------------------------------------------------------\n",
            "Fold 1: 0.6102 (61.02%) | Train: 2015-02-20 to 2016-10-07 | Test: 2016-10-17 to 2018-06-07\n",
            "Fold 2: 0.4867 (48.67%) | Train: 2015-02-20 to 2018-05-31 | Test: 2018-06-08 to 2020-01-29\n",
            "Fold 3: 0.5593 (55.93%) | Train: 2015-02-20 to 2020-01-22 | Test: 2020-01-30 to 2021-09-17\n",
            "Fold 4: 0.5157 (51.57%) | Train: 2015-02-20 to 2021-09-10 | Test: 2021-09-20 to 2023-05-10\n",
            "Fold 5: 0.5642 (56.42%) | Train: 2015-02-20 to 2023-05-03 | Test: 2023-05-11 to 2024-12-31\n",
            "\n",
            "======================================================================\n",
            "Overall Results for NVDA\n",
            "======================================================================\n",
            "Average Accuracy: 0.5472 (54.72%)\n",
            "Target Not Met: 5.28% below 60% target\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Down     0.4000    0.2235    0.2868       841\n",
            "          Up     0.5906    0.7696    0.6683      1224\n",
            "\n",
            "    accuracy                         0.5472      2065\n",
            "   macro avg     0.4953    0.4966    0.4776      2065\n",
            "weighted avg     0.5130    0.5472    0.5129      2065\n",
            "\n",
            "Downloading MSFT data from 2015-01-01 to 2025-01-01...\n",
            "\n",
            "======================================================================\n",
            "Training XGBOOST model for MSFT (Tier: tier3)\n",
            "======================================================================\n",
            "Total samples: 2,483\n",
            "Features: 16\n",
            "Date range: 2015-02-20 to 2024-12-31\n",
            "\n",
            "Class balance:\n",
            "  Down (0): 1,018 (41.0%)\n",
            "  Up (1): 1,465 (59.0%)\n",
            "\n",
            " High correlation pairs found (>0.9)\n",
            " - rsi <-> adx_diff: 0.941\n",
            " - bb_percent <-> stoch_k: 0.922\n",
            " - bb_percent <-> williams_r: 0.922\n",
            " - bb_percent <-> cci: 0.982\n",
            " - stoch_k <-> williams_r: 1.000\n",
            "\n",
            "Cross-validation with 5 folds (gap=5 days):\n",
            "----------------------------------------------------------------------\n",
            "Fold 1: 0.4092 (40.92%) | Train: 2015-02-20 to 2016-10-07 | Test: 2016-10-17 to 2018-06-07\n",
            "Fold 2: 0.5690 (56.90%) | Train: 2015-02-20 to 2018-05-31 | Test: 2018-06-08 to 2020-01-29\n",
            "Fold 3: 0.5714 (57.14%) | Train: 2015-02-20 to 2020-01-22 | Test: 2020-01-30 to 2021-09-17\n",
            "Fold 4: 0.4964 (49.64%) | Train: 2015-02-20 to 2021-09-10 | Test: 2021-09-20 to 2023-05-10\n",
            "Fold 5: 0.5327 (53.27%) | Train: 2015-02-20 to 2023-05-03 | Test: 2023-05-11 to 2024-12-31\n",
            "\n",
            "======================================================================\n",
            "Overall Results for MSFT\n",
            "======================================================================\n",
            "Average Accuracy: 0.5157 (51.57%)\n",
            "Target Not Met: 8.43% below 60% target\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Down     0.3796    0.3229    0.3490       830\n",
            "          Up     0.5865    0.6453    0.6145      1235\n",
            "\n",
            "    accuracy                         0.5157      2065\n",
            "   macro avg     0.4830    0.4841    0.4817      2065\n",
            "weighted avg     0.5033    0.5157    0.5078      2065\n",
            "\n",
            "Downloading JPM data from 2015-01-01 to 2025-01-01...\n",
            "\n",
            "======================================================================\n",
            "Training XGBOOST model for JPM (Tier: tier3)\n",
            "======================================================================\n",
            "Total samples: 2,483\n",
            "Features: 16\n",
            "Date range: 2015-02-20 to 2024-12-31\n",
            "\n",
            "Class balance:\n",
            "  Down (0): 1,074 (43.3%)\n",
            "  Up (1): 1,409 (56.7%)\n",
            "\n",
            " High correlation pairs found (>0.9)\n",
            " - rsi <-> adx_diff: 0.938\n",
            " - bb_percent <-> stoch_k: 0.910\n",
            " - bb_percent <-> williams_r: 0.910\n",
            " - bb_percent <-> cci: 0.984\n",
            " - stoch_k <-> williams_r: 1.000\n",
            "\n",
            "Cross-validation with 5 folds (gap=5 days):\n",
            "----------------------------------------------------------------------\n",
            "Fold 1: 0.4600 (46.00%) | Train: 2015-02-20 to 2016-10-07 | Test: 2016-10-17 to 2018-06-07\n",
            "Fold 2: 0.4770 (47.70%) | Train: 2015-02-20 to 2018-05-31 | Test: 2018-06-08 to 2020-01-29\n",
            "Fold 3: 0.5085 (50.85%) | Train: 2015-02-20 to 2020-01-22 | Test: 2020-01-30 to 2021-09-17\n",
            "Fold 4: 0.5327 (53.27%) | Train: 2015-02-20 to 2021-09-10 | Test: 2021-09-20 to 2023-05-10\n",
            "Fold 5: 0.5133 (51.33%) | Train: 2015-02-20 to 2023-05-03 | Test: 2023-05-11 to 2024-12-31\n",
            "\n",
            "======================================================================\n",
            "Overall Results for JPM\n",
            "======================================================================\n",
            "Average Accuracy: 0.4983 (49.83%)\n",
            "Target Not Met: 10.17% below 60% target\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Down     0.4118    0.3950    0.4032       886\n",
            "          Up     0.5588    0.5759    0.5673      1179\n",
            "\n",
            "    accuracy                         0.4983      2065\n",
            "   macro avg     0.4853    0.4855    0.4852      2065\n",
            "weighted avg     0.4957    0.4983    0.4969      2065\n",
            "\n",
            "Downloading AMZN data from 2015-01-01 to 2025-01-01...\n",
            "\n",
            "======================================================================\n",
            "Training XGBOOST model for AMZN (Tier: tier3)\n",
            "======================================================================\n",
            "Total samples: 2,483\n",
            "Features: 16\n",
            "Date range: 2015-02-20 to 2024-12-31\n",
            "\n",
            "Class balance:\n",
            "  Down (0): 1,058 (42.6%)\n",
            "  Up (1): 1,425 (57.4%)\n",
            "\n",
            " High correlation pairs found (>0.9)\n",
            " - rsi <-> adx_diff: 0.950\n",
            " - bb_percent <-> stoch_k: 0.916\n",
            " - bb_percent <-> williams_r: 0.916\n",
            " - bb_percent <-> cci: 0.984\n",
            " - stoch_k <-> williams_r: 1.000\n",
            "\n",
            "Cross-validation with 5 folds (gap=5 days):\n",
            "----------------------------------------------------------------------\n",
            "Fold 1: 0.5157 (51.57%) | Train: 2015-02-20 to 2016-10-07 | Test: 2016-10-17 to 2018-06-07\n",
            "Fold 2: 0.4939 (49.39%) | Train: 2015-02-20 to 2018-05-31 | Test: 2018-06-08 to 2020-01-29\n",
            "Fold 3: 0.5714 (57.14%) | Train: 2015-02-20 to 2020-01-22 | Test: 2020-01-30 to 2021-09-17\n",
            "Fold 4: 0.5036 (50.36%) | Train: 2015-02-20 to 2021-09-10 | Test: 2021-09-20 to 2023-05-10\n",
            "Fold 5: 0.5593 (55.93%) | Train: 2015-02-20 to 2023-05-03 | Test: 2023-05-11 to 2024-12-31\n",
            "\n",
            "======================================================================\n",
            "Overall Results for AMZN\n",
            "======================================================================\n",
            "Average Accuracy: 0.5288 (52.88%)\n",
            "Target Not Met: 7.12% below 60% target\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Down     0.4440    0.3404    0.3853       896\n",
            "          Up     0.5711    0.6732    0.6180      1169\n",
            "\n",
            "    accuracy                         0.5288      2065\n",
            "   macro avg     0.5075    0.5068    0.5017      2065\n",
            "weighted avg     0.5159    0.5288    0.5170      2065\n",
            "\n",
            "Downloading XOM data from 2015-01-01 to 2025-01-01...\n",
            "\n",
            "======================================================================\n",
            "Training XGBOOST model for XOM (Tier: tier3)\n",
            "======================================================================\n",
            "Total samples: 2,483\n",
            "Features: 16\n",
            "Date range: 2015-02-20 to 2024-12-31\n",
            "\n",
            "Class balance:\n",
            "  Down (0): 1,172 (47.2%)\n",
            "  Up (1): 1,311 (52.8%)\n",
            "\n",
            " High correlation pairs found (>0.9)\n",
            " - rsi <-> adx_diff: 0.940\n",
            " - bb_percent <-> stoch_k: 0.930\n",
            " - bb_percent <-> williams_r: 0.930\n",
            " - bb_percent <-> cci: 0.984\n",
            " - stoch_k <-> williams_r: 1.000\n",
            "\n",
            "Cross-validation with 5 folds (gap=5 days):\n",
            "----------------------------------------------------------------------\n",
            "Fold 1: 0.4915 (49.15%) | Train: 2015-02-20 to 2016-10-07 | Test: 2016-10-17 to 2018-06-07\n",
            "Fold 2: 0.5472 (54.72%) | Train: 2015-02-20 to 2018-05-31 | Test: 2018-06-08 to 2020-01-29\n",
            "Fold 3: 0.4867 (48.67%) | Train: 2015-02-20 to 2020-01-22 | Test: 2020-01-30 to 2021-09-17\n",
            "Fold 4: 0.5908 (59.08%) | Train: 2015-02-20 to 2021-09-10 | Test: 2021-09-20 to 2023-05-10\n",
            "Fold 5: 0.4915 (49.15%) | Train: 2015-02-20 to 2023-05-03 | Test: 2023-05-11 to 2024-12-31\n",
            "\n",
            "======================================================================\n",
            "Overall Results for XOM\n",
            "======================================================================\n",
            "Average Accuracy: 0.5215 (52.15%)\n",
            "Target Not Met: 7.85% below 60% target\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Down     0.4946    0.4178    0.4529       979\n",
            "          Up     0.5396    0.6151    0.5749      1086\n",
            "\n",
            "    accuracy                         0.5215      2065\n",
            "   macro avg     0.5171    0.5164    0.5139      2065\n",
            "weighted avg     0.5182    0.5215    0.5171      2065\n",
            "\n",
            "Downloading BAC data from 2015-01-01 to 2025-01-01...\n",
            "\n",
            "======================================================================\n",
            "Training XGBOOST model for BAC (Tier: tier3)\n",
            "======================================================================\n",
            "Total samples: 2,483\n",
            "Features: 16\n",
            "Date range: 2015-02-20 to 2024-12-31\n",
            "\n",
            "Class balance:\n",
            "  Down (0): 1,141 (46.0%)\n",
            "  Up (1): 1,342 (54.0%)\n",
            "\n",
            " High correlation pairs found (>0.9)\n",
            " - rsi <-> adx_diff: 0.946\n",
            " - bb_percent <-> stoch_k: 0.923\n",
            " - bb_percent <-> williams_r: 0.923\n",
            " - bb_percent <-> cci: 0.987\n",
            " - stoch_k <-> williams_r: 1.000\n",
            "\n",
            "Cross-validation with 5 folds (gap=5 days):\n",
            "----------------------------------------------------------------------\n",
            "Fold 1: 0.4891 (48.91%) | Train: 2015-02-20 to 2016-10-07 | Test: 2016-10-17 to 2018-06-07\n",
            "Fold 2: 0.5448 (54.48%) | Train: 2015-02-20 to 2018-05-31 | Test: 2018-06-08 to 2020-01-29\n",
            "Fold 3: 0.4794 (47.94%) | Train: 2015-02-20 to 2020-01-22 | Test: 2020-01-30 to 2021-09-17\n",
            "Fold 4: 0.4697 (46.97%) | Train: 2015-02-20 to 2021-09-10 | Test: 2021-09-20 to 2023-05-10\n",
            "Fold 5: 0.5133 (51.33%) | Train: 2015-02-20 to 2023-05-03 | Test: 2023-05-11 to 2024-12-31\n",
            "\n",
            "======================================================================\n",
            "Overall Results for BAC\n",
            "======================================================================\n",
            "Average Accuracy: 0.4993 (49.93%)\n",
            "Target Not Met: 10.07% below 60% target\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Down     0.4403    0.3904    0.4138       935\n",
            "          Up     0.5388    0.5894    0.5630      1130\n",
            "\n",
            "    accuracy                         0.4993      2065\n",
            "   macro avg     0.4896    0.4899    0.4884      2065\n",
            "weighted avg     0.4942    0.4993    0.4954      2065\n",
            "\n",
            "Downloading JNJ data from 2015-01-01 to 2025-01-01...\n",
            "\n",
            "======================================================================\n",
            "Training XGBOOST model for JNJ (Tier: tier3)\n",
            "======================================================================\n",
            "Total samples: 2,483\n",
            "Features: 16\n",
            "Date range: 2015-02-20 to 2024-12-31\n",
            "\n",
            "Class balance:\n",
            "  Down (0): 1,176 (47.4%)\n",
            "  Up (1): 1,307 (52.6%)\n",
            "\n",
            " High correlation pairs found (>0.9)\n",
            " - rsi <-> adx_diff: 0.928\n",
            " - bb_percent <-> stoch_k: 0.925\n",
            " - bb_percent <-> williams_r: 0.925\n",
            " - bb_percent <-> cci: 0.982\n",
            " - stoch_k <-> williams_r: 1.000\n",
            "\n",
            "Cross-validation with 5 folds (gap=5 days):\n",
            "----------------------------------------------------------------------\n",
            "Fold 1: 0.4673 (46.73%) | Train: 2015-02-20 to 2016-10-07 | Test: 2016-10-17 to 2018-06-07\n",
            "Fold 2: 0.5351 (53.51%) | Train: 2015-02-20 to 2018-05-31 | Test: 2018-06-08 to 2020-01-29\n",
            "Fold 3: 0.5351 (53.51%) | Train: 2015-02-20 to 2020-01-22 | Test: 2020-01-30 to 2021-09-17\n",
            "Fold 4: 0.4528 (45.28%) | Train: 2015-02-20 to 2021-09-10 | Test: 2021-09-20 to 2023-05-10\n",
            "Fold 5: 0.4964 (49.64%) | Train: 2015-02-20 to 2023-05-03 | Test: 2023-05-11 to 2024-12-31\n",
            "\n",
            "======================================================================\n",
            "Overall Results for JNJ\n",
            "======================================================================\n",
            "Average Accuracy: 0.4973 (49.73%)\n",
            "Target Not Met: 10.27% below 60% target\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Down     0.4707    0.3889    0.4259       990\n",
            "          Up     0.5148    0.5972    0.5530      1075\n",
            "\n",
            "    accuracy                         0.4973      2065\n",
            "   macro avg     0.4927    0.4930    0.4894      2065\n",
            "weighted avg     0.4937    0.4973    0.4920      2065\n",
            "\n",
            "\n",
            "==================================================\n",
            "Generating Final Reports...\n",
            "==================================================\n",
            "\n",
            "Detailed results saved to: detailed_results.csv\n",
            "\n",
            "======================================================================\n",
            "FINAL SUMMARY - ALL STOCKS\n",
            "======================================================================\n",
            "Ticker   Model  Tier Avg Acc Std Dev    Min    Max Target Met\n",
            "   SPY XGBOOST TIER3  52.59%   3.80% 46.25% 57.63%          ✗\n",
            "   QQQ XGBOOST TIER3  54.82%   2.96% 51.33% 59.08%          ✗\n",
            "  AAPL XGBOOST TIER3  51.28%   5.17% 42.37% 58.11%          ✗\n",
            "  NVDA XGBOOST TIER3  54.72%   4.25% 48.67% 61.02%          ✗\n",
            "  MSFT XGBOOST TIER3  51.57%   5.99% 40.92% 57.14%          ✗\n",
            "   JPM XGBOOST TIER3  49.83%   2.62% 46.00% 53.27%          ✗\n",
            "  AMZN XGBOOST TIER3  52.88%   3.09% 49.39% 57.14%          ✗\n",
            "   XOM XGBOOST TIER3  52.15%   4.12% 48.67% 59.08%          ✗\n",
            "   BAC XGBOOST TIER3  49.93%   2.70% 46.97% 54.48%          ✗\n",
            "   JNJ XGBOOST TIER3  49.73%   3.39% 45.28% 53.51%          ✗\n",
            "\n",
            "======================================================================\n",
            "Overall Statistics\n",
            "======================================================================\n",
            "Average Accuracy Across All Stocks: 51.95% (±1.77%)\n",
            "Stocks Meeting 60% Target: 0/10\n",
            "Best Performing Stock: QQQ (54.82%)\n",
            "Worst Performing Stock: JNJ (49.73%)\n",
            "\n",
            "Overall average is 8.05% below 60% target\n",
            "Recommendations:\n",
            "   - Try tier3 indicators for more features\n",
            "   - Test Random Forest model as alternative\n",
            "   - Consider ensemble methods (combine XGBoost + RF)\n",
            "\n",
            "Execution Complete! Check the './plots' folder for images.\n"
          ]
        }
      ],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}